{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adafb77",
   "metadata": {},
   "source": [
    "# Step 1 - Select S&P500 Dataset and Schiller P/E-10 Dataset from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import color\n",
    "from skimage.transform import rescale, resize,downscale_local_mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Data of S&P 500\")\n",
    "dataframe = pd.read_csv(\"SP500DATA.csv\")\n",
    "print(dataframe)\n",
    "print(\"Data of PE Ration\")\n",
    "data_pe = pd.read_csv('S&P500 Schiller PE ratio.csv')\n",
    "print(data_pe)\n",
    "test=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a319a5",
   "metadata": {},
   "source": [
    "# Step 2 - Develop and test/verify a correlation-based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pe = data_pe[[\"DateTime\"]].to_numpy(dtype = \"str\")\n",
    "date_sp = dataframe[[\"Date\"]].to_numpy(dtype = \"str\")\n",
    "\n",
    "date_pe = np.flip(date_pe)\n",
    "date_sp = np.flip(date_sp)\n",
    "\n",
    "\n",
    "ratio_sp = data_pe['SP500PERatio'].to_numpy(dtype = 'float')\n",
    "mean_sp = ratio_sp.mean()\n",
    "max_sp = ratio_sp.max()\n",
    "median_sp = np.median(ratio_sp)\n",
    "print(mean_sp)\n",
    "print(max_sp)\n",
    "print(median_sp)\n",
    "\n",
    "sp_index = dataframe[[\"Close/Last\", \"Open\"]].to_numpy(dtype = 'float')\n",
    "mean_sp_index = sp_index[:,0].mean()\n",
    "mean_sp_index1 = sp_index[:,1].mean()\n",
    "\n",
    "print(mean_sp_index)\n",
    "print(mean_sp_index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the data of S$P500 and PE ration\n",
    "record = []\n",
    "recordDate = []\n",
    "\n",
    "# get the date from data\n",
    "def get_date(date_data):\n",
    "    res = (date_data[:-5]).split('/')\n",
    "    if(len(res)==3):\n",
    "        return res\n",
    "    return None\n",
    "\n",
    "array_date_pe = date_pe.tolist()\n",
    "\n",
    "# if the S&P and PE in the same year and month, merge it into an item.\n",
    "\n",
    "for i in range(len(date_sp)):\n",
    "    tmp = [0 for _ in range(3)]\n",
    "    tmp[0] = sp_index[i][0]\n",
    "    record_time = str(date_sp[i][0])\n",
    "    for j in range(len(ratio_sp)):\n",
    "        peDate = get_date( array_date_pe[j][0] )\n",
    "\n",
    "        if(peDate is not None and len(peDate)==3) :\n",
    "            if(record_time[6:10]==peDate[2] ):\n",
    "                    if(peDate[1]==record_time[0:2] or ('0'+peDate[1])==record_time[0:2]):\n",
    "                        tmp[1] = ratio_sp[j]\n",
    "                        recordDate.append(record_time)\n",
    "                        record.append(tmp)\n",
    "\n",
    "dataset = np.array(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation scores\n",
    "\n",
    "sampleNum = 100\n",
    "for i in range(sampleNum-1 ,len(dataset)):\n",
    "    # Pearson  r\n",
    "    x = dataset[(i-sampleNum+1):(i+1), 0]\n",
    "    y = dataset[(i-sampleNum+1):(i+1), 1]\n",
    "    r = np.corrcoef(x, y)\n",
    "    dataset[i][2] = r[0,1] \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc850e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDigital = [];\n",
    "\n",
    "for i in range(len(recordDate)):\n",
    "    dateDigital.append(float(recordDate[i][6:10]) + ( (float(recordDate[i][0:2])-1)*30 + float(recordDate[i][3:5]) )/366 )\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "plt.plot(dateDigital[(sampleNum-1):],\n",
    "         dataset[(sampleNum-1):, 2], color='grey', label='Close')\n",
    "plt.title('The Correlation of S&P 500 close price and Shiller PE Ratio in 100 days')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buy the sp when r>threshold\n",
    "money = 1\n",
    "money_all = []\n",
    "trade_all = []\n",
    "# Must buy the new stock before sell\n",
    "lastsell=0\n",
    "\n",
    "for i in range(sampleNum-1, dataset.shape[0]):\n",
    "    corr_temp = dataset[i,2]\n",
    "    price_temp = dataset[(i-sampleNum+1):(i+1), 0]\n",
    "    if corr_temp > 0.5 and (i-lastsell > sampleNum):\n",
    "        # sell at this point\n",
    "        # buy at the local minia in this window interval (nsample interval)\n",
    "        buy_price = min(price_temp)\n",
    "        # in case of the overflow\n",
    "        if buy_price > 0:           \n",
    "            sell_price = price_temp[len(price_temp)-1]\n",
    "            money = sell_price/buy_price*money\n",
    "            lastsell = i\n",
    "            print(buy_price)\n",
    "            print(sell_price)\n",
    "            print(money)\n",
    "            print(i)\n",
    "            money_all.append(money)\n",
    "            trade_all.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431df3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5));\n",
    "#plt.plot(dataframe.Date, dataframe.Open.values, color='grey', label='Open')\n",
    "# plt.plot(dataframe.Date, dataframe.Close.values, color='black', label='Close')\n",
    "#plt.plot(dataframe.Date, dataframe.Low.values, color='red', label='Low')\n",
    "#plt.plot(dataframe.Date, dataframe.High.values, color='green', label='High')\n",
    "plt.plot(dataframe.Date, dataframe[\"Close/Last\"].values, color = 'red', label='Price')\n",
    "plt.title('S&P 500 price and Shiller PE Ratio')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('S&P 500 price')\n",
    "plt.legend(loc='best')\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=plt.twinx()\n",
    "ax2.set_xticks(ax2.get_xticks()[::100])\n",
    "plt.xlim(-31.1,500.1)\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data_pe.DateTime, data_pe.SP500PERatio.values,color=\"blue\")\n",
    "ax2.set_ylabel(\"Shiller PE Ratio\", color=\"blue\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db15e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot\n",
    "\n",
    "date_trade = []\n",
    "for i in range(len(trade_all)):\n",
    "    date_trade.append(recordDate[i])\n",
    "    \n",
    "plt.figure(figsize=(15, 6))\n",
    "print(len(date_trade))\n",
    "print(len(money_all))\n",
    "plt.plot(date_trade,\n",
    "         money_all, color='black', label='Close')\n",
    "plt.title('Money left at the end of trading (using correlation algorithm)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Money/millions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1459c",
   "metadata": {},
   "source": [
    "# Step 3 - Design and develop a convolutional neural network (CNN) in Python that will recognize A-B-C-D-wave patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge if their exist a A-B-C-D wave, if exist, save it.\n",
    "sellSigIdx = []\n",
    "prev = 0\n",
    "for x in range(sampleNum-1, len(dataset)):\n",
    "    if dataset[x][2] > 0.5 and x >= prev+sampleNum//3 and \\\n",
    "    dataset[x][0] == max(dataset[(x-sampleNum+1):(x+1), 0]) and \\\n",
    "    dataset[x][0] > dataset[x+1][0]:\n",
    "        sellSigIdx.append(x)\n",
    "        prev = x\n",
    "sellSigIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class CNet(nn.Module):\n",
    "    def __init__(self, linear_input=60):\n",
    "        super(CNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=20,padding=2,padding_mode='replicate', kernel_size=5, stride=1)\n",
    "        self.max_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(linear_input, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if(test==True):\n",
    "            x = x.permute(0,2,1)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 1) \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x =  self.ReLU( self.max_pool1(x) ) \n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.ReLU(self.linear1(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.cnn = CNet()\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.cnn(x) \n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "    \n",
    "        out = self.fc(out)\n",
    "    \n",
    "        return out[:, -1, :], hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f61b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainData = dataframe[[\"Close/Last\", \"Open\",'High','Low']].to_numpy(dtype = 'float')\n",
    "targetData = dataframe[[\"Close/Last\"]].to_numpy(dtype = 'float')\n",
    "date = dataframe[[\"Date\"]].to_numpy(dtype = \"str\")\n",
    "\n",
    "norm = MinMaxScaler(feature_range = (0, 1))\n",
    "trainData = norm.fit_transform(trainData)\n",
    "targetData = norm.fit_transform(targetData)\n",
    "trainData = np.flip(trainData)\n",
    "targetData = np.flip(targetData)\n",
    "date = np.flip(date)\n",
    "dateData = []\n",
    "for i in range(date.shape[0]):\n",
    "    dateData.append(float(np.squeeze(date)[i][6:10]) + ( (float(np.squeeze(date)[i][0:2])-1)*30 + float(np.squeeze(date)[i][3:5]) )/366 )\n",
    "dateData = np.array(dateData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 6\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(trainData.shape[0] - window_size - 4):\n",
    "    X.append(trainData[i:i+window_size])\n",
    "    Y.append(targetData[i+window_size:i+window_size+4])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = np.swapaxes(Y, 1, 2)\n",
    "dateAxis = dateData[:-(window_size+4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499363cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fullTraining = []\n",
    "X_training = []\n",
    "X_validation = []\n",
    "X_test = []\n",
    "Y_fullTraining = []\n",
    "Y_training = []\n",
    "Y_validation = []\n",
    "Y_test = []\n",
    "\n",
    "\n",
    "# 20% for test\n",
    "\n",
    "# For the left 80% data, 80% * 80% =64% for train, 80% * 20% = 16% for valdation \n",
    "X_fullTraining, X_test, Y_fullTraining, Y_test, date_ft, date_t = train_test_split(X, Y, dateAxis, test_size=0.2, shuffle=False)\n",
    "X_training, X_validation, Y_training, Y_validation, date_tr, date_v = train_test_split(X_fullTraining, Y_fullTraining, date_ft, test_size=0.2, shuffle=False)\n",
    "date_test=date[438:623,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = Model(input_size=4, output_size=4, hidden_dim=16, n_layers=1)\n",
    "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebe6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTensor = torch.from_numpy(X_training).float()\n",
    "xTensor_val = torch.from_numpy(X_validation).float()\n",
    "yTensor = torch.Tensor(Y_training).float()\n",
    "yTensor_val = torch.from_numpy(Y_validation).float()\n",
    "epochs  =150\n",
    "\n",
    "record_epoch = []\n",
    "record_loss = []\n",
    "record_loss_val = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, hidden = RNN(xTensor)\n",
    "    output_val, _ = RNN(xTensor_val)\n",
    "    loss = criterion(output.reshape(-1), yTensor.view(-1))\n",
    "    loss_val = criterion(output_val.reshape(-1), yTensor_val.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    record_epoch.append(epoch)\n",
    "    record_loss.append(loss.item())\n",
    "    record_loss_val.append(loss_val.item())\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "        print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4586b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6], dpi=80)\n",
    "plt.plot(np.array(record_loss), 'r', label = 'train')\n",
    "plt.plot(np.array(record_loss_val), 'b', label = 'val')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSEloss\")\n",
    "plt.legend()\n",
    "#plt.savefig('MSEloss_1.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82aaa6",
   "metadata": {},
   "source": [
    "# Step 4 - Train your CNN and RNN on your Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_error(pred, actual):\n",
    "    return abs(pred - actual)/pred\n",
    "\n",
    "# Calculate the accuracy, make a plot\n",
    "def plotGraphs(output_1,output_2,output_3,output_4,Y_main,Y_plot,XAxis_date,date_pred,\n",
    "    set1_title_1,set2_title_1,set3_title_1,set4_title_1,error_title):\n",
    "    \n",
    "    table = []\n",
    "\n",
    "    error = prediction_error(output_1, Y_main)\n",
    "    error_dt = pd.DataFrame(data = error)\n",
    "    temp1 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    temp1 = np.array(temp1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set1_title_1)\n",
    "    print(date_pred.shape)\n",
    "    print(output_1.shape)\n",
    "    ax1.plot(date_pred, output_1, color = 'blue', label = 'Pred')\n",
    "    ax1.plot(XAxis_date, Y_plot, color = 'green', label = 'Actual')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    ax2.plot(date_pred, error, color = 'red', label = 'Pred error')\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    error = prediction_error(output_2, Y_main)\n",
    "    error_dt = pd.DataFrame(data = error)\n",
    "    temp2 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    temp2 = np.array(temp2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set2_title_1)\n",
    "    ax1.plot(date_pred, output_2, color = 'blue', label = 'Pred')\n",
    "    ax1.plot(XAxis_date, Y_plot, color = 'green', label = 'Actual')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    ax2.plot(date_pred, error, color = 'red', label = 'Pred error')\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    error = prediction_error(output_3, Y_main)\n",
    "    error_dt = pd.DataFrame(data = error)\n",
    "    temp3 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    temp3 = np.array(temp3)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set3_title_1)\n",
    "    ax1.plot(date_pred, output_3, color = 'blue', label = 'Pred')\n",
    "    ax1.plot(XAxis_date, Y_plot, color = 'green', label = 'Actual')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    ax2.plot(date_pred, error, color = 'red', label = 'Pred error')\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    error = prediction_error(output_4, Y_main)\n",
    "    error_dt = pd.DataFrame(data = error)\n",
    "    temp4 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    temp4 = np.array(temp4)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set4_title_1)\n",
    "    ax1.plot(date_pred, output_plt4, color = 'blue', label = 'Pred')\n",
    "    ax1.plot(XAxis_date, Y_plot, color = 'green', label = 'Actual')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    ax2.plot(date_pred, error, color = 'red', label = 'Pred error')\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    temp1 = np.expand_dims(temp1, axis=1)\n",
    "    temp2 = np.expand_dims(temp2, axis=1)\n",
    "    temp3 = np.expand_dims(temp3, axis=1)\n",
    "    temp4 = np.expand_dims(temp4, axis=1)\n",
    "    table = np.concatenate((temp1, temp2, temp3, temp4), axis=1)\n",
    "    ind = ['Mean', 'STD-dev', 'Skewness', 'Kurtosis']\n",
    "    col = ['1 day', '2 day', '3 day', '4 day']\n",
    "    stats=pd.DataFrame(table, columns=col, index=ind).T\n",
    "    display(stats)\n",
    "    \n",
    "\n",
    "# Calculate the accuracy, make a plot\n",
    "def plotGraphsSD(output_1,output_2,output_3,output_4,Y_main,Y_plot,XAxis_date,date_pred,\n",
    "    set1_title_1,set2_title_1,set3_title_1,set4_title_1,error_title,sd):\n",
    "    \n",
    "    cmaps= ['red','purple', 'green', 'orange',  'yellow', 'pink', 'grey', 'brown','blue' ]\n",
    "    \n",
    "    table = []\n",
    "    error = []\n",
    "    \n",
    "    for index,threshold in enumerate(sd):\n",
    "        error.append(prediction_error(output_1[index], Y_main))\n",
    "    #error_dt = pd.DataFrame(data = error)\n",
    "    #temp1 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    #temp1 = np.array(temp1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set1_title_1)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        print(np.squeeze(output_1[index]))\n",
    "        ax1.plot(date_pred, np.squeeze(output_1[index]), color = cmaps[index], label = sd[index])\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax2.plot(np.squeeze(date_pred), error[index], color = cmaps[index], label = sd[index])\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    error = []\n",
    "    for index,threshold in enumerate(sd):\n",
    "        error.append(prediction_error(output_2[index], Y_main))\n",
    "    #error_dt = pd.DataFrame(data = error)\n",
    "    #temp2 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    #temp2 = np.array(temp2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set2_title_1)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax1.plot(date_pred, np.squeeze(output_2[index]), color = cmaps[index], label = sd[index])\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax2.plot(np.squeeze(date_pred), error[index], color = cmaps[index], label = sd[index])\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    error = []\n",
    "    for index,threshold in enumerate(sd):\n",
    "        error.append(prediction_error(output_3[index], Y_main))\n",
    "    #error_dt = pd.DataFrame(data = error)\n",
    "    #temp3 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    #temp3 = np.array(temp3)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set3_title_1)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax1.plot(date_pred, np.squeeze(output_3[index]), color = cmaps[index], label = sd[index])\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax2.plot(np.squeeze(date_pred), error[index], color = cmaps[index], label = sd[index])\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "    \n",
    "    for index,threshold in enumerate(sd):\n",
    "        error.append(prediction_error(output_4[index], Y_main))\n",
    "    #error_dt = pd.DataFrame(data = error)\n",
    "    #temp4 = [np.mean(error), np.std(error), error_dt.skew(axis=0).iloc[0], error_dt.kurtosis(axis=0).iloc[0]]\n",
    "    #temp4 = np.array(temp4)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(set4_title_1)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax1.plot(date_pred, np.squeeze(output_4[index]), color = cmaps[index], label = sd[index])\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.set_title(error_title)\n",
    "    for index,threshold in enumerate(sd):\n",
    "        ax2.plot(np.squeeze(date_pred), error[index], color = cmaps[index], label = sd[index])\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    # temp1 = np.expand_dims(temp1, axis=1)\n",
    "    # temp2 = np.expand_dims(temp2, axis=1)\n",
    "    # temp3 = np.expand_dims(temp3, axis=1)\n",
    "    # temp4 = np.expand_dims(temp4, axis=1)\n",
    "    # table = np.concatenate((temp1, temp2, temp3, temp4), axis=1)\n",
    "    # ind = ['Mean', 'STD-dev', 'Skewness', 'Kurtosis']\n",
    "    # col = ['1 day', '2 day', '3 day', '4 day']\n",
    "    # stats=pd.DataFrame(table, columns=col, index=ind).T\n",
    "    # display(stats)\n",
    "\n",
    "\n",
    "def plotAccuracy(output_1,output_2,output_3,output_4,Y_main, date_pred, title_content ):\n",
    "    \n",
    "    table = []\n",
    "\n",
    "    error_1 = prediction_error(output_1, Y_main)\n",
    "    error_2 = prediction_error(output_2, Y_main)\n",
    "    error_3 = prediction_error(output_3, Y_main)\n",
    "    error_4 = prediction_error(output_4, Y_main)\n",
    "    \n",
    "    fig = plt.figure(figsize=(9, 5))\n",
    "    plt.title(title_content)\n",
    "    plt.plot(date_pred, error_1, color = 'blue', label = 't+1')\n",
    "    plt.plot(date_pred, error_2, color = 'green', label = 't+2')\n",
    "    plt.plot(date_pred, error_3, color = 'purple', label = 't+3')\n",
    "    plt.plot(date_pred, error_4, color = 'red', label = 't+4')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed62ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plt = output.detach().numpy()\n",
    "\n",
    "output_plt = output_plt\n",
    "\n",
    "Y_plt = norm.inverse_transform(Y[:, :, 0])\n",
    "\n",
    "print(Y_plt.shape)\n",
    "Y_fullTrainingplt = norm.inverse_transform(Y_fullTraining[:, :, 0])\n",
    "\n",
    "output_plt1 = norm.inverse_transform(output_plt[:,0].reshape(-1,1))\n",
    "output_plt2 = norm.inverse_transform(output_plt[:,1].reshape(-1,1))\n",
    "output_plt3 = norm.inverse_transform(output_plt[:,2].reshape(-1,1))\n",
    "output_plt4 = norm.inverse_transform(output_plt[:,3].reshape(-1,1))\n",
    "\n",
    "print(output_plt1.shape)\n",
    "print(Y_training.shape)\n",
    "Y_output = Y_training[:,:,0]\n",
    "\n",
    "Y_testplt = norm.inverse_transform(Y_test[:, :, 0])\n",
    "\n",
    "#plotGraphs(output_plt1,output_plt2,output_plt3,output_plt4,Y_testplt,Y_plt,dateAxis,date_t,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plotGraphs(output_plt1,output_plt2,output_plt3,output_plt4,Y_output,Y_plt,dateAxis,date_tr,\n",
    "\"Full Training Set (1 day)\",\"Full Training Set (2 day)\",\"Full Training Set (3 day)\",\"Full Training Set (4 day)\",\"Error of Training Set\")\n",
    "\n",
    "plotAccuracy(output_plt1,output_plt2,output_plt3,output_plt4,Y_output,date_tr, 'Prediction Accuracy for price in future 1-4 days(On Train Data)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58c073",
   "metadata": {},
   "source": [
    "# Step 5 - Test your CNN and RNN accuracy over Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, make a plot\n",
    "\n",
    "def prediction_error(pred, actual):\n",
    "    return abs(pred - actual)/pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN.eval()\n",
    "xTensor = torch.from_numpy(X_test).float()\n",
    "output, _ = RNN(xTensor)\n",
    "\n",
    "output_plt1 = output.detach().numpy()[:, 0]\n",
    "output_plt1 = norm.inverse_transform(output_plt1.reshape(-1,1))\n",
    "\n",
    "output_plt2 = output.detach().numpy()[:, 1]\n",
    "output_plt2 = norm.inverse_transform(output_plt2.reshape(-1,1))\n",
    "\n",
    "output_plt3 = output.detach().numpy()[:, 2]\n",
    "output_plt3 = norm.inverse_transform(output_plt3.reshape(-1,1))\n",
    "\n",
    "output_plt4 = output.detach().numpy()[:, 3]\n",
    "output_plt4 = norm.inverse_transform(output_plt4.reshape(-1,1))\n",
    "\n",
    "Y_testplt = norm.inverse_transform(Y_test[:, :, 0])\n",
    "\n",
    "plotGraphs(output_plt1,output_plt2,output_plt3,output_plt4,Y_testplt,Y_plt,dateAxis,date_t,\n",
    "\"Test Set (1 day)\",\"Test Set (2 day)\",\"Test Set (3 day)\",\"Test Set (4 day)\",\"Error of Test Set\")\n",
    "\n",
    "plotAccuracy(output_plt1,output_plt2,output_plt3,output_plt4,Y_testplt,date_t, 'Prediction Accuracy for price P in future 1-4 days(On Test Data)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1652a",
   "metadata": {},
   "source": [
    "# Step 6 - Optimize your CNN+RNN algorithm's performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a82eac",
   "metadata": {},
   "source": [
    "### Adjust the parameter of the network and enhance the sliding window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "output_size =4\n",
    "hidden_dim = 32\n",
    "n_layers = 2\n",
    "epochs = 200\n",
    "window_size = 100\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(trainData.shape[0] - window_size - 4):\n",
    "    X.append(trainData[i:i+window_size])\n",
    "    Y.append(targetData[i+window_size:i+window_size+4])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = np.swapaxes(Y, 1, 2)\n",
    "X_fullTraining = []\n",
    "X_training = []\n",
    "X_validation = []\n",
    "X_test = []\n",
    "Y_fullTraining = []\n",
    "Y_training = []\n",
    "Y_validation = []\n",
    "Y_test = []\n",
    "\n",
    "dateAxis = []\n",
    "dateData = []\n",
    "for i in range(date.shape[0]):\n",
    "    dateData.append(float(np.squeeze(date)[i][6:10]) + ( (float(np.squeeze(date)[i][0:2])-1)*30 + float(np.squeeze(date)[i][3:5]) )/366 )\n",
    "dateData = np.array(dateData)\n",
    "dateAxis = dateData[(window_size+4):]\n",
    "\n",
    "\n",
    "X_fullTraining, X_test, Y_fullTraining, Y_test, date_ft, date_t = train_test_split(X, Y, dateAxis, test_size=0.2, shuffle=False)\n",
    "X_training, X_validation, Y_training, Y_validation, date_tr, date_v = train_test_split(X_fullTraining, Y_fullTraining, date_ft, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_enhance = Model(input_size, output_size, hidden_dim, n_layers)\n",
    "RNN_enhance.cnn = CNet(50*20)\n",
    "optimizer = torch.optim.Adam(RNN_enhance.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "xTensor = torch.from_numpy(X_training).float()\n",
    "yTensor = torch.Tensor(Y_training).float()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, hidden = RNN_enhance(xTensor)\n",
    "    loss = criterion(output.reshape(-1), yTensor.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "        print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f612af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN_enhance.eval()\n",
    "xTensor = torch.from_numpy(X_test).float()\n",
    "output, _ = RNN_enhance(xTensor)\n",
    "\n",
    "Y_plt = norm.inverse_transform(Y[:, :, 0])\n",
    "\n",
    "output_plt1 = output.detach().numpy()[:, 0]\n",
    "output_plt1 = norm.inverse_transform(output_plt1.reshape(-1,1))\n",
    "\n",
    "output_plt2 = output.detach().numpy()[:, 1]\n",
    "output_plt2 = norm.inverse_transform(output_plt2.reshape(-1,1))\n",
    "\n",
    "output_plt3 = output.detach().numpy()[:, 2]\n",
    "output_plt3 = norm.inverse_transform(output_plt3.reshape(-1,1))\n",
    "\n",
    "output_plt4 = output.detach().numpy()[:, 3]\n",
    "output_plt4 = norm.inverse_transform(output_plt4.reshape(-1,1))\n",
    "\n",
    "Y_testplt = norm.inverse_transform(Y_test[:, :, 0])\n",
    "\n",
    "plotGraphs(output_plt1,output_plt2,output_plt3,output_plt4,Y_testplt,Y_plt,dateAxis,date_t,\n",
    "\"Test Set (1 day)\",\"Test Set (2 day)\",\"Test Set (3 day)\",\"Test Set (4 day)\",\"Error of Test Set\")\n",
    "\n",
    "plotAccuracy(output_plt1,output_plt2,output_plt3,output_plt4,Y_testplt,date_t,'Prediction Accuracy for price in future 1-4 days(Optimized )' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c5b78",
   "metadata": {},
   "source": [
    "# Step 7 - Enhance your CNN+RNN algorithm's functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee147758",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_fullTraining.shape)\n",
    "# start with $1 m \n",
    "money = 1\n",
    "money_all = []\n",
    "trade_all = []\n",
    "date_all = []\n",
    "money_all.append(money)\n",
    "date_all.append(dateDigital[0])\n",
    "sampleNum = 6\n",
    "# Must buy the new stock before sell\n",
    "lastsell = -sampleNum\n",
    "\n",
    "for i in range(1,X_fullTraining.shape[0]):\n",
    "    tmp = norm.inverse_transform(X_fullTraining[i,:, :])\n",
    "    \n",
    "    prob_temp = tmp[:, 2]\n",
    "    price_temp = tmp[:, 0]\n",
    "    \n",
    "    # Sell when has high prob\n",
    "    if dataset[i][2] > 0.9 and (i-lastsell > sampleNum):\n",
    "        \n",
    "        \n",
    "        # The buy at a low PE ration\n",
    "        \n",
    "        buy_price = min(price_temp)\n",
    "        \n",
    "        if buy_price > 0:\n",
    "            sell_price = price_temp[len(price_temp)-1]\n",
    "            \n",
    "            money*= (sell_price/buy_price)\n",
    "            \n",
    "            lastsell = i\n",
    "            money_all.append(money)\n",
    "            trade_all.append(i)\n",
    "            date_all.append(dateDigital[i])\n",
    "            \n",
    "            print(buy_price)\n",
    "            print(sell_price)\n",
    "            print(money)\n",
    "            print(i)\n",
    "plt.figure(figsize=(12, 6));\n",
    "plt.scatter(date_all,\n",
    "         money_all, color='green', label='Close')\n",
    "plt.yticks(money_all)\n",
    "plt.title('Enhanced CNN+RNN Money Return')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Money (in millions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e9007",
   "metadata": {},
   "source": [
    "# Step 8 - Determine the Effect of Input Data Perturbations(Price Uncertainty) on your Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44d7bf",
   "metadata": {},
   "source": [
    "## 8a) Using your Optimized CNN+RNN algorithm (from Step 6, above) corrupt your Test Set that you used in Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68921b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noisy(dataset, threshold):\n",
    "    dataset_noise = np.ndarray(shape=dataset.shape, dtype=np.float32)\n",
    "    #set random seed \n",
    "    random.seed(2)\n",
    "    cut_nums = 4\n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        s = np.random.normal(0, threshold, cut_nums)\n",
    "        idx = random.sample(list(range(window_size)), cut_nums)\n",
    "        dataset_noise[i] = dataset[i] \n",
    "        \n",
    "        #scale the s by the sampling window\n",
    "        s = max(X_test[1, idx, 0])*s\n",
    "        tmp = dataset_noise[i, idx, 0] + s\n",
    "        # modify the data when negative\n",
    "        tmp[tmp < 0] = 1e-7\n",
    "        dataset_noise[i, idx, 0]  = tmp\n",
    "        \n",
    "    return dataset_noise   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9bb03",
   "metadata": {},
   "source": [
    "## 8b) Repeat Tests in Step 6, above, with noise-corrupted Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next 1 day: idx=0 \n",
    "\n",
    "std = [0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1 ]\n",
    "\n",
    "idx = 0\n",
    "out_tmp = np.concatenate( (Y_test[:, :, idx], Y_test[:, :, idx], Y_test[:, :, idx]), axis = 1)\n",
    "target_test = norm.inverse_transform(out_tmp)[:, [0]]\n",
    "error_res_all = []\n",
    "output_plt1_all=[]\n",
    "output_plt2_all=[]\n",
    "output_plt3_all=[]\n",
    "output_plt4_all=[]\n",
    "\n",
    "for threshold in std:\n",
    "    dataset_test_noise = add_noisy(X_test, threshold)\n",
    "    test_inputs = Variable(torch.from_numpy(dataset_test_noise).float())\n",
    "    output_test, _ = RNN_enhance(test_inputs)\n",
    "    out_tmp = np.concatenate((output_test.detach().numpy()[:, [idx]], \n",
    "                               output_test.detach().numpy()[:, [idx]],  \n",
    "                               output_test.detach().numpy()[:, [idx]]), axis=1)\n",
    "    \n",
    "    predicted_price_test = norm.inverse_transform(out_tmp)[:, [0]]\n",
    "    error = prediction_error(target_test, predicted_price_test)\n",
    "    error_res_all.append(error.reshape(-1))\n",
    "\n",
    "    output_plt1 = output_test.detach().numpy()[:, 0]\n",
    "    output_plt1 = norm.inverse_transform(output_plt1.reshape(-1,1))\n",
    "    output_plt1_all.append(output_plt1)\n",
    "\n",
    "    output_plt2 = output_test.detach().numpy()[:, 1]\n",
    "    output_plt2 = norm.inverse_transform(output_plt2.reshape(-1,1))\n",
    "    output_plt2_all.append(output_plt2)\n",
    "\n",
    "    output_plt3 = output_test.detach().numpy()[:, 2]\n",
    "    output_plt3 = norm.inverse_transform(output_plt3.reshape(-1,1))\n",
    "    output_plt3_all.append(output_plt3)\n",
    "\n",
    "    output_plt4 = output_test.detach().numpy()[:, 3]\n",
    "    output_plt4 = norm.inverse_transform(output_plt4.reshape(-1,1))\n",
    "    output_plt4_all.append(output_plt4)\n",
    "\n",
    "Y_plt = norm.inverse_transform(Y[:, :, 0])\n",
    "\n",
    "Y_testplt = norm.inverse_transform(Y_test[:, :, 0])\n",
    "\n",
    "plotGraphsSD(output_plt1_all,output_plt2_all,output_plt3_all,output_plt4_all,Y_testplt,Y_plt,dateAxis,date_t,\n",
    "\"Test Set (1 day)\",\"Test Set (2 day)\",\"Test Set (3 day)\",\"Test Set (4 day)\",\"Error of Test Set\",std)\n",
    "\n",
    "# plotAccuracy(output_plt1_all,output_plt2_all,output_plt3_all,output_plt4_all,Y_testplt,np.squeeze(date_t),'Prediction Accuracy for price in future 1-4 days(Optimized )' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b4d00",
   "metadata": {},
   "source": [
    "## 8c) Repeat Step 7, above, to Determine How Much Money You Make or Lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0a247",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_noise1D(outputVector,stdev):\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape)\n",
    "    return outputVector + noise\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "cmaps= ['red','purple', 'green', 'orange',  'yellow', 'pink', 'grey', 'brown','blue' ]\n",
    "for index,threshold in enumerate(std):\n",
    "    # Redoing step 2 in order to add noise to ratio_sp, and thus dataset (which is used in step 7)\n",
    "    print(threshold)\n",
    "    ratio_sp_noise = add_noise1D(ratio_sp, threshold)\n",
    "    \n",
    "    record_noise = []\n",
    "    recordDate_noise = []\n",
    "    \n",
    "    for i in range(len(date_sp)):\n",
    "        tmp = [0 for _ in range(3)]\n",
    "        tmp[0] = sp_index[i][0]\n",
    "        record_time = str(date_sp[i][0])\n",
    "        for j in range(len(ratio_sp)):\n",
    "            peDate = get_date( array_date_pe[j][0] )\n",
    "\n",
    "            if(peDate is not None and len(peDate)==3) :\n",
    "                if(record_time[6:10]==peDate[2] ):\n",
    "                        if(peDate[1]==record_time[0:2] or ('0'+peDate[1])==record_time[0:2]):\n",
    "                            tmp[1] = ratio_sp_noise[j]\n",
    "                            recordDate_noise.append(record_time)\n",
    "                            record_noise.append(tmp)\n",
    "    \n",
    "    dataset_noise = np.array(record_noise)\n",
    "    sampleNum = 100\n",
    "    for i in range(sampleNum-1 ,len(dataset)):\n",
    "        # Pearson  r\n",
    "        x = dataset_noise[(i-sampleNum+1):(i+1), 0]\n",
    "        y = dataset_noise[(i-sampleNum+1):(i+1), 1]\n",
    "        r = np.corrcoef(x, y)\n",
    "        dataset_noise[i][2] = r[0,1] # Here is where we create noisy data used for step 8c\n",
    "\n",
    "    # start with $1 m \n",
    "    money = 1\n",
    "    money_all = []\n",
    "    trade_all = []\n",
    "    date_all = []\n",
    "    money_all.append(money)\n",
    "    date_all.append(dateDigital[0])\n",
    "    sampleNum = 6\n",
    "    # Must buy the new stock before sell\n",
    "    lastsell = -sampleNum\n",
    "\n",
    "    for i in range(1,X_fullTraining.shape[0]):\n",
    "        tmp = norm.inverse_transform(X_fullTraining[i,:, :])\n",
    "\n",
    "        prob_temp = tmp[:, 2]\n",
    "        price_temp = tmp[:, 0]\n",
    "\n",
    "        # Sell when has high prob\n",
    "        if dataset_noise[i][2] > 0.9 and (i-lastsell > sampleNum): # Just repeating step 7, but using dataset_noise\n",
    "\n",
    "\n",
    "            # The buy at a low PE ration\n",
    "\n",
    "            buy_price = min(price_temp)\n",
    "\n",
    "            if buy_price > 0:\n",
    "                sell_price = price_temp[len(price_temp)-1]\n",
    "\n",
    "                money*= (sell_price/buy_price)\n",
    "\n",
    "                lastsell = i\n",
    "                money_all.append(money.reshape(-1))\n",
    "                trade_all.append(i)\n",
    "                date_all.append(dateDigital[i])\n",
    "    \n",
    "    \n",
    "    plt.scatter(np.squeeze(date_all), money_all, color=cmaps[index], label=std[index])\n",
    "    plt.yticks(money_all)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('Enhanced CNN+RNN Money Return')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Money (in millions)')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab3be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_fullTraining.shape)\n",
    "# start with $1 m \n",
    "money = 1\n",
    "money_all = []\n",
    "trade_all = []\n",
    "sampleNum = 6\n",
    "# Must buy the new stock before sell\n",
    "lastsell = -sampleNum\n",
    "\n",
    "for i in range(1,test_inputs.shape[0]):\n",
    "    tmp = norm.inverse_transform(test_inputs[i,:, :])\n",
    "    \n",
    "    prob_temp = tmp[:, 2]\n",
    "    price_temp = tmp[:, 0]\n",
    "    \n",
    "    # Sell when has high prob\n",
    "    if dataset[i][2] > 0.9 and (i-lastsell > sampleNum):\n",
    "        \n",
    "        \n",
    "        # The buy at a low PE ration\n",
    "        \n",
    "        buy_price = min(price_temp)\n",
    "        \n",
    "        if buy_price > 0:\n",
    "            sell_price = price_temp[len(price_temp)-1]\n",
    "            \n",
    "            money*= (sell_price/buy_price)\n",
    "            \n",
    "            lastsell = i\n",
    "            money_all.append(money)\n",
    "            trade_all.append(i)\n",
    "            \n",
    "            print(buy_price)\n",
    "            print(sell_price)\n",
    "            print(money)\n",
    "            print(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade14f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaps= ['red','orange',  'yellow', 'green','blue', 'purple', 'pink', 'grey', 'brown', 'black' ]\n",
    "plt.figure(1, figsize=(12, 6))\n",
    "for i in range(len(std)):   \n",
    "    plt.plot(date_test[trade_res_all[i]], money_res_all[i] , color=cmaps[i], lw=1, label = sd[i], marker= 'o')\n",
    "    \n",
    "plt.ylabel('Money/millions')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.title('Money Earned for CNN+RNN Model with Noise-Corrupted Test Data <2012-2020> (Optimized)')\n",
    "plt.legend(loc=\"upper left\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af93c7d",
   "metadata": {},
   "source": [
    "# Step 9 - Modify the CNN+RNN algorithm in this assignment to generate a BUY signal as well as a SELL signal.  Then optimize your enhanced algorithm as stated in Steps 5 and 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb79757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b702ee99c54dc3b4d4b3705c725497563f4c969def566813e3a469d09f4b3fc1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
