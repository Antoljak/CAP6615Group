{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------PART 1-------\n",
      "\n",
      "\n",
      "Checking accuracy on Training Set\n",
      "\n",
      "FH: 0.9819819819819819 FFA: 0.14482758620689656\n",
      "FH: 0.9770114942528736 FFA: 0.09467455621301775\n",
      "FH: 0.888268156424581 FFA: 0.0\n",
      "FH: 0.8421052631578947 FFA: 0.011764705882352941\n",
      "FH: 0.8291139240506329 FFA: 0.0\n",
      "FH: 0.8988095238095238 FFA: 0.011363636363636364\n",
      "FH: 0.8959537572254336 FFA: 0.024096385542168676\n",
      "FH: 0.948905109489051 FFA: 0.04201680672268908\n",
      "FH: 0.8762886597938144 FFA: 0.03225806451612903\n",
      "FH: 0.8663101604278075 FFA: 0.0\n",
      "FH: 0.9664429530201343 FFA: 0.037383177570093455\n",
      "FH: 0.9153439153439153 FFA: 0.0\n",
      "FH: 0.9363057324840764 FFA: 0.010101010101010102\n",
      "FH: 0.9263157894736842 FFA: 0.0\n",
      "FH: 0.8348623853211009 FFA: 0.0\n",
      "FH: 0.8900523560209425 FFA: 0.0\n",
      "FH: 0.8306878306878307 FFA: 0.0\n",
      "FH: 0.8913043478260869 FFA: 0.013888888888888888\n",
      "FH: 1.0 FFA: 0.04794520547945205\n",
      "FH: 0.96875 FFA: 0.1328125\n",
      "FH: 0.9720670391061452 FFA: 0.06493506493506493\n",
      "FH: 0.98125 FFA: 0.0\n",
      "FH: 0.8294930875576036 FFA: 0.02564102564102564\n",
      "FH: 0.8341708542713567 FFA: 0.0\n",
      "FH: 0.9454545454545454 FFA: 0.01098901098901099\n",
      "FH: 0.9269662921348315 FFA: 0.0\n",
      "FH: 0.9215686274509803 FFA: 0.07766990291262135\n",
      "FH: 0.9045226130653267 FFA: 0.0\n",
      "FH: 0.8 FFA: 0.014084507042253521\n",
      "FH: 0.9485294117647058 FFA: 0.016666666666666666\n",
      "FH: 0.9243243243243243 FFA: 0.0\n",
      "FH: 0.9403973509933775 FFA: 0.0761904761904762\n",
      "FH: 0.9887005649717514 FFA: 0.0\n",
      "FH: 1.0 FFA: 0.0449438202247191\n",
      "FH: 0.9772727272727273 FFA: 0.008064516129032258\n",
      "FH: 0.8977272727272727 FFA: 0.0\n",
      "\n",
      "Checking accuracy on Test Set\n",
      "\n",
      "FH: 0.9615384615384616 FFA: 0.2894736842105263\n",
      "FH: 1.0 FFA: 0.3248730964467005\n",
      "FH: 0.8547008547008547 FFA: 0.45323741007194246\n",
      "FH: 0.8854961832061069 FFA: 0.432\n",
      "FH: 0.8409090909090909 FFA: 0.31547619047619047\n",
      "FH: 0.9349593495934959 FFA: 0.48120300751879697\n",
      "FH: 0.9225352112676056 FFA: 0.30701754385964913\n",
      "FH: 0.9456521739130435 FFA: 0.27439024390243905\n",
      "FH: 0.9050632911392406 FFA: 0.3469387755102041\n",
      "FH: 0.888 FFA: 0.33587786259541985\n",
      "FH: 0.8790322580645161 FFA: 0.3181818181818182\n",
      "FH: 0.9285714285714286 FFA: 0.3137254901960784\n",
      "FH: 0.864 FFA: 0.3969465648854962\n",
      "FH: 0.8481012658227848 FFA: 0.29591836734693877\n",
      "FH: 0.8592592592592593 FFA: 0.5454545454545454\n",
      "FH: 0.8661417322834646 FFA: 0.5348837209302325\n",
      "FH: 0.9383561643835616 FFA: 0.39090909090909093\n",
      "FH: 0.8631578947368421 FFA: 0.30303030303030304\n",
      "FH: 0.9620253164556962 FFA: 0.2711864406779661\n",
      "FH: 0.9495798319327731 FFA: 0.27007299270072993\n",
      "FH: 0.9408284023668639 FFA: 0.367816091954023\n",
      "FH: 0.905511811023622 FFA: 0.37209302325581395\n",
      "FH: 0.8758169934640523 FFA: 0.4077669902912621\n",
      "FH: 0.9212598425196851 FFA: 0.4806201550387597\n",
      "FH: 0.9596774193548387 FFA: 0.2727272727272727\n",
      "FH: 0.9006622516556292 FFA: 0.45714285714285713\n",
      "FH: 0.8774193548387097 FFA: 0.2079207920792079\n",
      "FH: 0.9333333333333333 FFA: 0.41509433962264153\n",
      "FH: 0.8853503184713376 FFA: 0.3838383838383838\n",
      "FH: 0.8796992481203008 FFA: 0.13008130081300814\n",
      "FH: 0.9405940594059405 FFA: 0.4645161290322581\n",
      "FH: 0.9326923076923077 FFA: 0.2894736842105263\n",
      "FH: 0.8939393939393939 FFA: 0.4112903225806452\n",
      "FH: 0.9333333333333333 FFA: 0.371900826446281\n",
      "FH: 0.9090909090909091 FFA: 0.17777777777777778\n",
      "FH: 0.9333333333333333 FFA: 0.38016528925619836\n",
      "\n",
      "NOISY\n",
      "\n",
      "False\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7387387387387387\n",
      "FFA: 0.6206896551724138\n",
      "FH: 0.8735632183908046\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8659217877094972\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.7953216374269005\n",
      "FFA: 0.4235294117647059\n",
      "FH: 0.6835443037974683\n",
      "FFA: 0.6530612244897959\n",
      "FH: 0.7678571428571429\n",
      "FFA: 0.48863636363636365\n",
      "FH: 0.6647398843930635\n",
      "FFA: 0.6867469879518072\n",
      "FH: 0.8978102189781022\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6521739130434783\n",
      "\n",
      "-------PART 2--------\n",
      "\n",
      "FH: 0.0 FFA: 0.1409185803757829\n",
      "FH: 0.04597701149425287 FFA: 0.1423529411764706\n",
      "FH: 0.10416666666666667 FFA: 0.13520408163265307\n",
      "FH: 0.017699115044247787 FFA: 0.14270032930845225\n",
      "FH: 0.06153846153846154 FFA: 0.13764337851929093\n",
      "FH: 0.02040816326530612 FFA: 0.1459521094640821\n",
      "FH: 0.0 FFA: 0.15587529976019185\n",
      "FH: 0.0 FFA: 0.14300736067297581\n",
      "FH: 0.0 FFA: 0.15349369988545247\n",
      "FH: 0.0 FFA: 0.14891179839633448\n",
      "FH: 0.009900990099009901 FFA: 0.13867822318526543\n",
      "FH: 0.0 FFA: 0.15121412803532008\n",
      "FH: 0.04 FFA: 0.1396103896103896\n",
      "FH: 0.012195121951219513 FFA: 0.14118895966029724\n",
      "FH: 0.13147410358565736 FFA: 0.12807244501940493\n",
      "FH: 0.0 FFA: 0.14960629921259844\n",
      "FH: 0.005115089514066497 FFA: 0.20221169036334913\n",
      "FH: 0.038461538461538464 FFA: 0.1358695652173913\n",
      "FH: 0.01818181818181818 FFA: 0.13209494324045407\n",
      "FH: 0.14285714285714285 FFA: 0.1243680485338726\n",
      "FH: 0.043478260869565216 FFA: 0.12460732984293194\n",
      "FH: 0.017241379310344827 FFA: 0.13768115942028986\n",
      "FH: 0.022727272727272728 FFA: 0.1346938775510204\n",
      "FH: 0.039603960396039604 FFA: 0.13434452871072589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def predict(inputVector, weightMatrix):\n",
    "        '''\n",
    "        Predict the output of the input vector.\n",
    "        '''\n",
    "        \n",
    "        predictionVector = []\n",
    "        for i in range(len(weightMatrix)):\n",
    "            row_sum = sum(\n",
    "                (inputVector[j] * weightMatrix[j][i])\n",
    "                for j in range(len(weightMatrix[0]) - 1)\n",
    "            )\n",
    "\n",
    "            predictionVector.append(self.activation_fn(row_sum))\n",
    "           \n",
    "        return predictionVector\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, inputImageVectors):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    tableObject = {}\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "\n",
    "    for i in range(len(stdev)):\n",
    "        tableObject['std_'+ str(stdev[i]) + '_fh'] = []\n",
    "        tableObject['std_'+ str(stdev[i]) + '_ffa'] = []\n",
    "\n",
    "    for j in range(len(stdev)):   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],0.1,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if output[l] > 0:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(inputImageVectors[k],testPrediction)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "def Create_Image_Set(filename, ASL):\n",
    "    ImageVectors = []\n",
    "    if filename!='ASL32x':\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    if filename!='ASL32x':\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    elif filename=='ASL32x':\n",
    "        for i in range(24):\n",
    "            x = i + 65\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    elif ASL==True:\n",
    "        for i in range(34):\n",
    "            x = i + 65\n",
    "            if i!=19 and i>10:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            plt.imshow(im)\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "set1 = Create_Image_Set('characters1',False)\n",
    "set2 = Create_Image_Set('characters2',False)\n",
    "set3 = Create_Image_Set('ASL32x',False)\n",
    "set1mod = Create_Image_Set('characters1',True)\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    for i in range(256):\n",
    "        if outputVector[i] < 0.0001:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n------PART 1-------\\n\")\n",
    "print(\"\\nChecking accuracy on Training Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nChecking accuracy on Test Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"\\nNOISY\\n\")\n",
    "print(part2)\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 10, set1)\n",
    "print(\"\\n-------PART 2--------\\n\")\n",
    "imageTensor = torch.Tensor(set3)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(24):\n",
    "    output = model(torch.from_numpy(set1mod[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set3[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
