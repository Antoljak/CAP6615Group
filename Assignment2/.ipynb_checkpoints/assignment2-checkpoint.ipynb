{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------PART 1-------\n",
      "\n",
      "\n",
      "Checking accuracy on Training Set\n",
      "\n",
      "FH: 0.9819819819819819 FFA: 0.1310344827586207\n",
      "FH: 0.9770114942528736 FFA: 0.10650887573964497\n",
      "FH: 0.9106145251396648 FFA: 0.0\n",
      "FH: 0.8713450292397661 FFA: 0.011764705882352941\n",
      "FH: 0.8164556962025317 FFA: 0.01020408163265306\n",
      "FH: 0.875 FFA: 0.011363636363636364\n",
      "FH: 0.8959537572254336 FFA: 0.012048192771084338\n",
      "FH: 0.9343065693430657 FFA: 0.04201680672268908\n",
      "FH: 0.865979381443299 FFA: 0.08064516129032258\n",
      "FH: 0.8235294117647058 FFA: 0.0\n",
      "FH: 0.9865771812080537 FFA: 0.056074766355140186\n",
      "FH: 0.873015873015873 FFA: 0.029850746268656716\n",
      "FH: 0.9235668789808917 FFA: 0.13131313131313133\n",
      "FH: 0.9157894736842105 FFA: 0.0\n",
      "FH: 0.8623853211009175 FFA: 0.0\n",
      "FH: 0.9109947643979057 FFA: 0.0\n",
      "FH: 0.8306878306878307 FFA: 0.0\n",
      "FH: 0.9619565217391305 FFA: 0.027777777777777776\n",
      "FH: 0.9818181818181818 FFA: 0.03424657534246575\n",
      "FH: 0.953125 FFA: 0.078125\n",
      "FH: 0.9608938547486033 FFA: 0.025974025974025976\n",
      "FH: 0.99375 FFA: 0.010416666666666666\n",
      "FH: 0.9308755760368663 FFA: 0.0\n",
      "FH: 0.8844221105527639 FFA: 0.0\n",
      "FH: 0.9272727272727272 FFA: 0.0\n",
      "FH: 0.9719101123595506 FFA: 0.01282051282051282\n",
      "FH: 0.9084967320261438 FFA: 0.009708737864077669\n",
      "FH: 0.9195979899497487 FFA: 0.017543859649122806\n",
      "FH: 0.8216216216216217 FFA: 0.0\n",
      "FH: 0.9632352941176471 FFA: 0.03333333333333333\n",
      "FH: 0.972972972972973 FFA: 0.0\n",
      "FH: 0.9933774834437086 FFA: 0.09523809523809523\n",
      "FH: 0.9887005649717514 FFA: 0.012658227848101266\n",
      "FH: 1.0 FFA: 0.11235955056179775\n",
      "FH: 0.9924242424242424 FFA: 0.04032258064516129\n",
      "FH: 0.9204545454545454 FFA: 0.0125\n",
      "\n",
      "Checking accuracy on Test Set\n",
      "\n",
      "FH: 0.9711538461538461 FFA: 0.2894736842105263\n",
      "FH: 1.0 FFA: 0.27918781725888325\n",
      "FH: 0.8632478632478633 FFA: 0.43884892086330934\n",
      "FH: 0.8473282442748091 FFA: 0.376\n",
      "FH: 0.8409090909090909 FFA: 0.34523809523809523\n",
      "FH: 0.8455284552845529 FFA: 0.45112781954887216\n",
      "FH: 0.8802816901408451 FFA: 0.3508771929824561\n",
      "FH: 0.9347826086956522 FFA: 0.3048780487804878\n",
      "FH: 0.9050632911392406 FFA: 0.3673469387755102\n",
      "FH: 0.856 FFA: 0.33587786259541985\n",
      "FH: 0.9032258064516129 FFA: 0.3333333333333333\n",
      "FH: 0.948051948051948 FFA: 0.37254901960784315\n",
      "FH: 0.896 FFA: 0.5267175572519084\n",
      "FH: 0.8481012658227848 FFA: 0.3673469387755102\n",
      "FH: 0.8888888888888888 FFA: 0.5619834710743802\n",
      "FH: 0.9291338582677166 FFA: 0.5348837209302325\n",
      "FH: 0.8972602739726028 FFA: 0.44545454545454544\n",
      "FH: 0.8789473684210526 FFA: 0.2878787878787879\n",
      "FH: 0.9620253164556962 FFA: 0.23163841807909605\n",
      "FH: 0.9159663865546218 FFA: 0.23357664233576642\n",
      "FH: 0.9289940828402367 FFA: 0.4367816091954023\n",
      "FH: 0.9212598425196851 FFA: 0.4263565891472868\n",
      "FH: 0.8758169934640523 FFA: 0.5533980582524272\n",
      "FH: 0.8818897637795275 FFA: 0.5426356589147286\n",
      "FH: 0.9112903225806451 FFA: 0.25757575757575757\n",
      "FH: 0.9205298013245033 FFA: 0.5142857142857142\n",
      "FH: 0.832258064516129 FFA: 0.21782178217821782\n",
      "FH: 0.9333333333333333 FFA: 0.37735849056603776\n",
      "FH: 0.9299363057324841 FFA: 0.43434343434343436\n",
      "FH: 0.8796992481203008 FFA: 0.14634146341463414\n",
      "FH: 0.9405940594059405 FFA: 0.6129032258064516\n",
      "FH: 0.9519230769230769 FFA: 0.3157894736842105\n",
      "FH: 0.9090909090909091 FFA: 0.5080645161290323\n",
      "FH: 0.9481481481481482 FFA: 0.3884297520661157\n",
      "FH: 0.9421487603305785 FFA: 0.17777777777777778\n",
      "FH: 0.9333333333333333 FFA: 0.371900826446281\n",
      "\n",
      "NOISY\n",
      "\n",
      "False\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5680473372781065\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8187134502923976\n",
      "FFA: 0.36470588235294116\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7380952380952381\n",
      "FFA: 0.5340909090909091\n",
      "FH: 0.6416184971098265\n",
      "FFA: 0.7228915662650602\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.3865546218487395\n",
      "FH: 0.7061855670103093\n",
      "FFA: 0.5483870967741935\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "\n",
      "-------PART 2--------\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 256 is out of bounds for axis 0 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-b246f87f9e9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mffa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_performance_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FH:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FFA:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mffa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-b246f87f9e9f>\u001b[0m in \u001b[0;36mcalculate_performance_metrics\u001b[1;34m(inputVector, outputVector, ASL)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0minputSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0moutputVector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[1;32mif\u001b[0m  \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputVector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minputVector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mrightBlackPixelCount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 256 is out of bounds for axis 0 with size 256"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(1024, 256),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128,128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128,256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def predict(inputVector, weightMatrix):\n",
    "        '''\n",
    "        Predict the output of the input vector.\n",
    "        '''\n",
    "        \n",
    "        predictionVector = []\n",
    "        for i in range(len(weightMatrix)):\n",
    "            row_sum = sum(\n",
    "                (inputVector[j] * weightMatrix[j][i])\n",
    "                for j in range(len(weightMatrix[0]) - 1)\n",
    "            )\n",
    "\n",
    "            predictionVector.append(self.activation_fn(row_sum))\n",
    "           \n",
    "        return predictionVector\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, inputImageVectors):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    tableObject = {}\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "\n",
    "    for i in range(len(stdev)):\n",
    "        tableObject['std_'+ str(stdev[i]) + '_fh'] = []\n",
    "        tableObject['std_'+ str(stdev[i]) + '_ffa'] = []\n",
    "\n",
    "    for j in range(len(stdev)):   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],0.1,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if output[l] > 0:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(inputImageVectors[k],testPrediction,False)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "def Create_Image_Set(filename, ASL):\n",
    "    ImageVectors = []\n",
    "    if filename!='ASL32x':\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    if filename!='ASL32x':\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    elif filename=='ASL32x':\n",
    "        for i in range(24):\n",
    "            x = i + 65\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    elif ASL==True:\n",
    "        for i in range(34):\n",
    "            x = i + 65\n",
    "            if i!=19 and i>10:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            plt.imshow(im)\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "set1 = Create_Image_Set('characters1',False)\n",
    "set2 = Create_Image_Set('characters2',False)\n",
    "set3 = Create_Image_Set('ASL32x',False)\n",
    "set1mod = Create_Image_Set('characters1',True)\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector, ASL):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    \n",
    "    if ASL==True:\n",
    "        inputSize=1024\n",
    "    else:\n",
    "        inputSize=256\n",
    "    for i in range(inputSize):\n",
    "        if outputVector[i] < 0.0001:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n------PART 1-------\\n\")\n",
    "print(\"\\nChecking accuracy on Training Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output, False)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nChecking accuracy on Test Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output, False)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"\\nNOISY\\n\")\n",
    "print(part2)\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 10, set1)\n",
    "print(\"\\n-------PART 2--------\\n\")\n",
    "imageTensor = torch.Tensor(set3)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(24):\n",
    "    output = model(torch.from_numpy(set3[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set3[i], output, True)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
