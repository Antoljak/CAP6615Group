{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\A.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\B.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\C.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\D.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\E.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\F.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\G.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\H.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\I.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\K.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\L.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\M.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\N.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\O.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\P.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\Q.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\R.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\S.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\T.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\U.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\V.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\W.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\X.png\n",
      "C:\\Users\\vjsol\\Desktop\\UF\\CAP6615\\CAP6615Group\\Assignment2\\characters1\\Y.png\n",
      "\n",
      "------PART 1-------\n",
      "\n",
      "\n",
      "Checking accuracy on Training Set\n",
      "\n",
      "FH: 0.972972972972973 FFA: 0.14482758620689656\n",
      "FH: 0.9540229885057471 FFA: 0.20710059171597633\n",
      "FH: 0.8938547486033519 FFA: 0.0\n",
      "FH: 0.9064327485380117 FFA: 0.023529411764705882\n",
      "FH: 0.810126582278481 FFA: 0.030612244897959183\n",
      "FH: 0.9107142857142857 FFA: 0.011363636363636364\n",
      "FH: 0.9017341040462428 FFA: 0.024096385542168676\n",
      "FH: 0.9416058394160584 FFA: 0.025210084033613446\n",
      "FH: 0.8195876288659794 FFA: 0.06451612903225806\n",
      "FH: 0.8770053475935828 FFA: 0.014492753623188406\n",
      "FH: 0.9530201342281879 FFA: 0.06542056074766354\n",
      "FH: 0.8624338624338624 FFA: 0.029850746268656716\n",
      "FH: 0.8980891719745223 FFA: 0.050505050505050504\n",
      "FH: 0.8842105263157894 FFA: 0.015151515151515152\n",
      "FH: 0.8532110091743119 FFA: 0.0\n",
      "FH: 0.8848167539267016 FFA: 0.0\n",
      "FH: 0.8412698412698413 FFA: 0.0\n",
      "FH: 0.9565217391304348 FFA: 0.0\n",
      "FH: 0.9818181818181818 FFA: 0.0547945205479452\n",
      "FH: 0.9453125 FFA: 0.140625\n",
      "FH: 0.9664804469273743 FFA: 0.025974025974025976\n",
      "FH: 0.975 FFA: 0.03125\n",
      "FH: 0.9078341013824884 FFA: 0.05128205128205128\n",
      "FH: 0.914572864321608 FFA: 0.0\n",
      "FH: 0.9393939393939394 FFA: 0.02197802197802198\n",
      "FH: 0.9775280898876404 FFA: 0.01282051282051282\n",
      "FH: 0.934640522875817 FFA: 0.038834951456310676\n",
      "FH: 0.9045226130653267 FFA: 0.0\n",
      "FH: 0.8324324324324325 FFA: 0.028169014084507043\n",
      "FH: 0.9632352941176471 FFA: 0.03333333333333333\n",
      "FH: 0.9513513513513514 FFA: 0.0\n",
      "FH: 0.9801324503311258 FFA: 0.12380952380952381\n",
      "FH: 0.943502824858757 FFA: 0.27848101265822783\n",
      "FH: 0.9880239520958084 FFA: 0.0898876404494382\n",
      "FH: 0.9924242424242424 FFA: 0.016129032258064516\n",
      "FH: 0.8977272727272727 FFA: 0.0\n",
      "\n",
      "Checking accuracy on Test Set\n",
      "\n",
      "FH: 0.9615384615384616 FFA: 0.27631578947368424\n",
      "FH: 0.9830508474576272 FFA: 0.3248730964467005\n",
      "FH: 0.8376068376068376 FFA: 0.460431654676259\n",
      "FH: 0.8702290076335878 FFA: 0.368\n",
      "FH: 0.8522727272727273 FFA: 0.3869047619047619\n",
      "FH: 0.8211382113821138 FFA: 0.45864661654135336\n",
      "FH: 0.9014084507042254 FFA: 0.32456140350877194\n",
      "FH: 0.9347826086956522 FFA: 0.31097560975609756\n",
      "FH: 0.8987341772151899 FFA: 0.41836734693877553\n",
      "FH: 0.904 FFA: 0.42748091603053434\n",
      "FH: 0.8709677419354839 FFA: 0.26515151515151514\n",
      "FH: 0.922077922077922 FFA: 0.39215686274509803\n",
      "FH: 0.872 FFA: 0.4732824427480916\n",
      "FH: 0.8987341772151899 FFA: 0.3163265306122449\n",
      "FH: 0.8888888888888888 FFA: 0.5785123966942148\n",
      "FH: 0.9212598425196851 FFA: 0.5193798449612403\n",
      "FH: 0.952054794520548 FFA: 0.5363636363636364\n",
      "FH: 0.868421052631579 FFA: 0.30303030303030304\n",
      "FH: 0.9620253164556962 FFA: 0.2711864406779661\n",
      "FH: 0.8487394957983193 FFA: 0.24087591240875914\n",
      "FH: 0.9289940828402367 FFA: 0.367816091954023\n",
      "FH: 0.905511811023622 FFA: 0.4806201550387597\n",
      "FH: 0.8627450980392157 FFA: 0.5436893203883495\n",
      "FH: 0.889763779527559 FFA: 0.5271317829457365\n",
      "FH: 0.9274193548387096 FFA: 0.23484848484848486\n",
      "FH: 0.9139072847682119 FFA: 0.49523809523809526\n",
      "FH: 0.8516129032258064 FFA: 0.21782178217821782\n",
      "FH: 0.88 FFA: 0.41509433962264153\n",
      "FH: 0.9044585987261147 FFA: 0.3939393939393939\n",
      "FH: 0.8796992481203008 FFA: 0.14634146341463414\n",
      "FH: 0.9207920792079208 FFA: 0.535483870967742\n",
      "FH: 0.9134615384615384 FFA: 0.2894736842105263\n",
      "FH: 0.8787878787878788 FFA: 0.4435483870967742\n",
      "FH: 0.9333333333333333 FFA: 0.35537190082644626\n",
      "FH: 0.9421487603305785 FFA: 0.17777777777777778\n",
      "FH: 0.9185185185185185 FFA: 0.3884297520661157\n",
      "\n",
      "NOISY\n",
      "\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "FH: 0.7207207207207207\n",
      "FFA: 0.6275862068965518\n",
      "FH: 0.8390804597701149\n",
      "FFA: 0.5798816568047337\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.18181818181818182\n",
      "FH: 0.8128654970760234\n",
      "FFA: 0.3764705882352941\n",
      "FH: 0.6708860759493671\n",
      "FFA: 0.6632653061224489\n",
      "FH: 0.7321428571428571\n",
      "FFA: 0.5454545454545454\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.7349397590361446\n",
      "FH: 0.927007299270073\n",
      "FFA: 0.3697478991596639\n",
      "FH: 0.7010309278350515\n",
      "FFA: 0.5645161290322581\n",
      "FH: 0.6737967914438503\n",
      "FFA: 0.6521739130434783\n",
      "\n",
      "-------PART 2--------\n",
      "\n",
      "Image tensor dimension: torch.Size([24, 1024])\n",
      "Result tensor dimension: torch.Size([24, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FH: 0.3624161073825503 FFA: 0.4672897196261682\n",
      "FH: 0.3968253968253968 FFA: 0.43283582089552236\n",
      "FH: 0.3885350318471338 FFA: 0.43434343434343436\n",
      "FH: 0.42105263157894735 FFA: 0.36363636363636365\n",
      "FH: 0.38990825688073394 FFA: 0.5\n",
      "FH: 0.418848167539267 FFA: 0.36923076923076925\n",
      "FH: 0.4126984126984127 FFA: 0.3880597014925373\n",
      "FH: 0.391304347826087 FFA: 0.4444444444444444\n",
      "FH: 0.42727272727272725 FFA: 0.3904109589041096\n",
      "FH: 0.39664804469273746 FFA: 0.42857142857142855\n",
      "FH: 0.40625 FFA: 0.40625\n",
      "FH: 0.4147465437788018 FFA: 0.358974358974359\n",
      "FH: 0.40703517587939697 FFA: 0.40350877192982454\n",
      "FH: 0.4121212121212121 FFA: 0.3956043956043956\n",
      "FH: 0.39325842696629215 FFA: 0.4358974358974359\n",
      "FH: 0.40522875816993464 FFA: 0.4077669902912621\n",
      "FH: 0.40703517587939697 FFA: 0.40350877192982454\n",
      "FH: 0.43783783783783786 FFA: 0.323943661971831\n",
      "FH: 0.4411764705882353 FFA: 0.36666666666666664\n",
      "FH: 0.4 FFA: 0.4225352112676056\n",
      "FH: 0.37748344370860926 FFA: 0.44761904761904764\n",
      "FH: 0.3333333333333333 FFA: 0.569620253164557\n",
      "FH: 0.39520958083832336 FFA: 0.42696629213483145\n",
      "FH: 0.4393939393939394 FFA: 0.3709677419354839\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(1024, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512,256),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(256,128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128,24),\n",
    "      nn.Softmax(dim=0),\n",
    "      nn.Linear(24,256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, inputImageVectors):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    tableObject = {}\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "\n",
    "    for i in range(len(stdev)):\n",
    "        tableObject['std_'+ str(stdev[i]) + '_fh'] = []\n",
    "        tableObject['std_'+ str(stdev[i]) + '_ffa'] = []\n",
    "\n",
    "    for j in range(len(stdev)):   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],0.1,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if output[l] > 0:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(inputImageVectors[k],testPrediction)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "def Create_Image_Set(filename, ASL):\n",
    "    ImageVectors = []\n",
    "    if filename!='ASL32x' and filename!='ASL64x' and ASL==False: #set1 and set2\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    elif (filename=='ASL32x' or filename=='ASL64x') and ASL==False: #set3\n",
    "        for i in range(25):\n",
    "            x = i + 65\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "                im = Image.open(path, 'r')\n",
    "                gray = im.convert('L')\n",
    "                bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "                ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    elif ASL==True: #set1mod\n",
    "        for i in range(25):\n",
    "            x = i + 65\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "                print(path)\n",
    "                im = Image.open(path, 'r')\n",
    "                gray = im.convert('L')\n",
    "                bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "                ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "set1 = Create_Image_Set('characters1',False)\n",
    "set2 = Create_Image_Set('characters2',False)\n",
    "set3 = Create_Image_Set('ASL32x',False)\n",
    "#set3 = Create_Image_Set('ASL64x',False)\n",
    "set1mod = Create_Image_Set('characters1',True)\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    \n",
    "    for i in range(256):\n",
    "        if outputVector[i] < 0.0001:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n------PART 1-------\\n\")\n",
    "print(\"\\nChecking accuracy on Training Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nChecking accuracy on Test Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"\\nNOISY\\n\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 10, set1)\n",
    "print(\"\\n-------PART 2--------\\n\")\n",
    "imageTensor = torch.Tensor(set3)\n",
    "resultTensor = torch.Tensor(set1mod)\n",
    "print(\"Image tensor dimension:\",imageTensor.size())\n",
    "print(\"Result tensor dimension:\", resultTensor.size())\n",
    "_dataSet = TensorDataset(imageTensor, resultTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP2()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "for i in range(24):\n",
    "    output = model(torch.from_numpy(set3[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1mod[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
