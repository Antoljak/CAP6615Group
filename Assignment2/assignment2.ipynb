{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64901c29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      #nn.Linear(4096,1024),\n",
    "      #nn.Sigmoid(),\n",
    "      nn.Linear(1024, 1024),\n",
    "      nn.Sigmoid(),\n",
    "      nn.Linear(1024,512),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(512,256),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Linear(256,128),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Linear(128,256),\n",
    "      nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, noiseCrossSection, inputImageVectors, outputVector):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    print(\"Noisy Results for Cross-section - \", noiseCrossSection)\n",
    "    for j in range(len(stdev)):\n",
    "        print(\"Std. Dev - \",stdev[j])   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],noiseCrossSection,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            # if(k == 0):\n",
    "            #     print(\"Noisy Input\",corruptedVector)\n",
    "            #     print(\"Noisy Output\", testPrediction)\n",
    "            for l in range(256):\n",
    "                if testPrediction[l] > 0.01:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(outputVector[k],testPrediction)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "def Create_Image_Set(filename, ASL):\n",
    "    ImageVectors = []\n",
    "    if filename!='ASL32x' and filename!='ASL64x' and ASL==False: #set1 and set2\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    elif (filename=='ASL32x' or filename=='ASL64x') and ASL==False: #set3\n",
    "        for i in range(25):\n",
    "            threshold = 0\n",
    "            if i==1 or i==2 or i==3 or i==5 or i==6 or i==7 or i==8 or i==9 or i==10 or i==12 or i==13 or i==14 or i==15 or i==16:\n",
    "                threshold=210\n",
    "            elif i==18:\n",
    "                threshold=210\n",
    "            else:\n",
    "                threshold=219\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "                im = Image.open(path, 'r')\n",
    "                gray = im.convert('L')\n",
    "                bw = gray.point(lambda x: 0 if x<threshold else 1, '1')\n",
    "                #plt.figure()\n",
    "                #plt.imshow(bw)\n",
    "                ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    elif ASL==True: #set1mod\n",
    "        for i in range(25):\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "                im = Image.open(path, 'r')\n",
    "                gray = im.convert('L')\n",
    "                bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "                ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "set1 = Create_Image_Set('characters1',False)\n",
    "set2 = Create_Image_Set('characters2',False)\n",
    "set3 = Create_Image_Set('ASL32x',False)\n",
    "#set3 = Create_Image_Set('ASL64x',False)\n",
    "set1mod = Create_Image_Set('characters1',True)\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    \n",
    "    for i in range(256):\n",
    "        if outputVector[i] == 0:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa\n",
    "\n",
    "def displayTrainingPredictions(predictionDataArray):\n",
    "    '''\n",
    "    Display the training predictions.\n",
    "    '''\n",
    "    f,axisArray = plt.subplots(len(predictionDataArray),len(predictionDataArray[0]),figsize=(10,10))\n",
    "    plt.suptitle('SLP Training Set performance')\n",
    "    for i in range(len(predictionDataArray)):\n",
    "        for j in range(len(predictionDataArray[0])):\n",
    "            axisArray[i,j].axes.xaxis.set_visible(False)\n",
    "            axisArray[i,j].axes.yaxis.set_visible(False)\n",
    "            if j == 0:\n",
    "                axisArray[i,j].set_title('Input', fontsize=8,x =0.5,y = 0.9)\n",
    "            else:\n",
    "                axisArray[i,j].set_title('Epoch - '+ str(j), fontsize=8, x =0.5,y = 0.9)\n",
    "            axisArray[i,j].imshow(self.getPredictedImage(predictionDataArray[i][j]))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "'''\n",
    "print(\"\\n------PART 1-------\\n\")\n",
    "print(\"\\nChecking accuracy on Training Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nChecking accuracy on Test Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"\\nNOISY\\n\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 36,0.1, set1)\n",
    "'''\n",
    "print(\"\\n-------PART 2--------\\n\")\n",
    "imageTensor = torch.Tensor(set3)\n",
    "resultTensor = torch.Tensor(set1mod)\n",
    "_dataSet = TensorDataset(imageTensor, resultTensor)\n",
    "_dataLoader = DataLoader(_dataSet,batch_size=2)\n",
    "\n",
    "model = MLP2()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        #print(\"Loss at this epoch: \",loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "for i in range(24):\n",
    "    output = model(torch.from_numpy(set3[i].astype('float32'))).detach().numpy()\n",
    "    #print(\"Output: \",output,\" Input: \",set3[i])\n",
    "    for j in range(256):\n",
    "        if output[j] > 0.01:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1mod[i], output)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(np.reshape(output,[16,16]))\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nNOISY\\n\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 24, 0.1, set3, set1mod)\n",
    "# Noisy_Testing(stdev, 24, 0.2, set3, set1mod)\n",
    "# Noisy_Testing(stdev, 24, 0.25, set3, set1mod)\n",
    "# Noisy_Testing(stdev, 24, 0.3, set3, set1mod)\n",
    "# Noisy_Testing(stdev, 24, 0.35, set3, set1mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a46c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2160e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61385f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1422863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00976ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
