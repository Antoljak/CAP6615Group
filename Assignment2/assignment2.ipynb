{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------PART 1-------\n",
      "\n",
      "\n",
      "Checking accuracy on Training Set\n",
      "\n",
      "FH: 0.9369369369369369 FFA: 0.15172413793103448\n",
      "FH: 0.9540229885057471 FFA: 0.14792899408284024\n",
      "FH: 0.888268156424581 FFA: 0.0\n",
      "FH: 0.9005847953216374 FFA: 0.07058823529411765\n",
      "FH: 0.8227848101265823 FFA: 0.04081632653061224\n",
      "FH: 0.875 FFA: 0.0\n",
      "FH: 0.8670520231213873 FFA: 0.0\n",
      "FH: 0.9343065693430657 FFA: 0.04201680672268908\n",
      "FH: 0.8350515463917526 FFA: 0.04838709677419355\n",
      "FH: 0.8609625668449198 FFA: 0.0\n",
      "FH: 0.9530201342281879 FFA: 0.037383177570093455\n",
      "FH: 0.873015873015873 FFA: 0.014925373134328358\n",
      "FH: 0.89171974522293 FFA: 0.010101010101010102\n",
      "FH: 0.8894736842105263 FFA: 0.0\n",
      "FH: 0.8073394495412844 FFA: 0.0\n",
      "FH: 0.8743455497382199 FFA: 0.0\n",
      "FH: 0.7724867724867724 FFA: 0.0\n",
      "FH: 0.9239130434782609 FFA: 0.013888888888888888\n",
      "FH: 0.9545454545454546 FFA: 0.0273972602739726\n",
      "FH: 0.96875 FFA: 0.09375\n",
      "FH: 0.9664804469273743 FFA: 0.0\n",
      "FH: 0.94375 FFA: 0.0625\n",
      "FH: 0.8894009216589862 FFA: 0.1794871794871795\n",
      "FH: 0.8542713567839196 FFA: 0.017543859649122806\n",
      "FH: 0.9212121212121213 FFA: 0.01098901098901099\n",
      "FH: 0.9550561797752809 FFA: 0.01282051282051282\n",
      "FH: 0.9477124183006536 FFA: 0.038834951456310676\n",
      "FH: 0.914572864321608 FFA: 0.0\n",
      "FH: 0.8162162162162162 FFA: 0.04225352112676056\n",
      "FH: 0.9705882352941176 FFA: 0.008333333333333333\n",
      "FH: 0.9405405405405406 FFA: 0.0\n",
      "FH: 0.9735099337748344 FFA: 0.0761904761904762\n",
      "FH: 0.9661016949152542 FFA: 0.08860759493670886\n",
      "FH: 0.9880239520958084 FFA: 0.02247191011235955\n",
      "FH: 0.9924242424242424 FFA: 0.008064516129032258\n",
      "FH: 0.8977272727272727 FFA: 0.0\n",
      "\n",
      "Checking accuracy on Test Set\n",
      "\n",
      "FH: 0.9423076923076923 FFA: 0.23684210526315788\n",
      "FH: 0.9661016949152542 FFA: 0.36548223350253806\n",
      "FH: 0.8376068376068376 FFA: 0.45323741007194246\n",
      "FH: 0.9083969465648855 FFA: 0.392\n",
      "FH: 0.8522727272727273 FFA: 0.375\n",
      "FH: 0.8292682926829268 FFA: 0.39849624060150374\n",
      "FH: 0.8591549295774648 FFA: 0.30701754385964913\n",
      "FH: 0.967391304347826 FFA: 0.2865853658536585\n",
      "FH: 0.8987341772151899 FFA: 0.336734693877551\n",
      "FH: 0.912 FFA: 0.37404580152671757\n",
      "FH: 0.9032258064516129 FFA: 0.25757575757575757\n",
      "FH: 0.8961038961038961 FFA: 0.21568627450980393\n",
      "FH: 0.864 FFA: 0.44274809160305345\n",
      "FH: 0.8924050632911392 FFA: 0.32653061224489793\n",
      "FH: 0.8296296296296296 FFA: 0.512396694214876\n",
      "FH: 0.84251968503937 FFA: 0.4573643410852713\n",
      "FH: 0.8356164383561644 FFA: 0.4818181818181818\n",
      "FH: 0.8473684210526315 FFA: 0.2727272727272727\n",
      "FH: 0.9493670886075949 FFA: 0.2598870056497175\n",
      "FH: 0.8823529411764706 FFA: 0.24087591240875914\n",
      "FH: 0.8698224852071006 FFA: 0.3563218390804598\n",
      "FH: 0.8740157480314961 FFA: 0.4573643410852713\n",
      "FH: 0.8431372549019608 FFA: 0.5533980582524272\n",
      "FH: 0.8582677165354331 FFA: 0.5193798449612403\n",
      "FH: 0.9274193548387096 FFA: 0.3333333333333333\n",
      "FH: 0.9072847682119205 FFA: 0.3904761904761905\n",
      "FH: 0.896774193548387 FFA: 0.22772277227722773\n",
      "FH: 0.8933333333333333 FFA: 0.32075471698113206\n",
      "FH: 0.8280254777070064 FFA: 0.42424242424242425\n",
      "FH: 0.8872180451127819 FFA: 0.14634146341463414\n",
      "FH: 0.8910891089108911 FFA: 0.5419354838709678\n",
      "FH: 0.9519230769230769 FFA: 0.375\n",
      "FH: 0.8712121212121212 FFA: 0.41935483870967744\n",
      "FH: 0.9407407407407408 FFA: 0.38016528925619836\n",
      "FH: 0.9090909090909091 FFA: 0.1925925925925926\n",
      "FH: 0.9185185185185185 FFA: 0.34710743801652894\n",
      "\n",
      "NOISY\n",
      "\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "FH: 0.7117117117117117\n",
      "FFA: 0.6\n",
      "FH: 0.8620689655172413\n",
      "FFA: 0.5384615384615384\n",
      "FH: 0.8603351955307262\n",
      "FFA: 0.15584415584415584\n",
      "FH: 0.8011695906432749\n",
      "FFA: 0.3411764705882353\n",
      "FH: 0.6645569620253164\n",
      "FFA: 0.6224489795918368\n",
      "FH: 0.7261904761904762\n",
      "FFA: 0.5\n",
      "FH: 0.6358381502890174\n",
      "FFA: 0.6746987951807228\n",
      "FH: 0.8832116788321168\n",
      "FFA: 0.37815126050420167\n",
      "FH: 0.6907216494845361\n",
      "FFA: 0.5161290322580645\n",
      "FH: 0.6631016042780749\n",
      "FFA: 0.6086956521739131\n",
      "\n",
      "-------PART 2--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def predict(inputVector, weightMatrix):\n",
    "        '''\n",
    "        Predict the output of the input vector.\n",
    "        '''\n",
    "        \n",
    "        predictionVector = []\n",
    "        for i in range(len(weightMatrix)):\n",
    "            row_sum = sum(\n",
    "                (inputVector[j] * weightMatrix[j][i])\n",
    "                for j in range(len(weightMatrix[0]) - 1)\n",
    "            )\n",
    "\n",
    "            predictionVector.append(self.activation_fn(row_sum))\n",
    "           \n",
    "        return predictionVector\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, inputImageVectors):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    tableObject = {}\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "\n",
    "    for i in range(len(stdev)):\n",
    "        tableObject['std_'+ str(stdev[i]) + '_fh'] = []\n",
    "        tableObject['std_'+ str(stdev[i]) + '_ffa'] = []\n",
    "\n",
    "    for j in range(len(stdev)):   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],0.1,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if output[l] > 0:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(inputImageVectors[k],testPrediction)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "def Create_Image_Set(filename):\n",
    "    ImageVectors = []\n",
    "    \n",
    "    if filename!='ASL32x':\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "    if filename!='ASL32x':\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    else:\n",
    "        for i in range(24):\n",
    "            x = i + 65\n",
    "            if i!=9:\n",
    "                path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "set1 = Create_Image_Set('characters1')\n",
    "set2 = Create_Image_Set('characters2')\n",
    "set3 = Create_Image_Set('ASL32x')\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    for i in range(256):\n",
    "        if outputVector[i] < 0.0001:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa \n",
    "\n",
    "model.eval()\n",
    "print(\"\\n------PART 1-------\\n\")\n",
    "print(\"\\nChecking accuracy on Training Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\nChecking accuracy on Test Set\\n\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"\\nNOISY\\n\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 10, set1)\n",
    "print(\"\\n-------PART 2--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
