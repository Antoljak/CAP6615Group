{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534b935f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Training Set\n",
      "FH: 1.0 FFA: 0.11724137931034483\n",
      "FH: 0.9195402298850575 FFA: 0.27218934911242604\n",
      "FH: 0.8994413407821229 FFA: 0.0\n",
      "FH: 0.8830409356725146 FFA: 0.011764705882352941\n",
      "FH: 0.8924050632911392 FFA: 0.04081632653061224\n",
      "FH: 0.8809523809523809 FFA: 0.022727272727272728\n",
      "FH: 0.884393063583815 FFA: 0.012048192771084338\n",
      "FH: 0.9781021897810219 FFA: 0.03361344537815126\n",
      "FH: 0.8608247422680413 FFA: 0.04838709677419355\n",
      "FH: 0.8663101604278075 FFA: 0.0\n",
      "FH: 0.9664429530201343 FFA: 0.037383177570093455\n",
      "FH: 0.8835978835978836 FFA: 0.014925373134328358\n",
      "FH: 0.9171974522292994 FFA: 0.010101010101010102\n",
      "FH: 0.9210526315789473 FFA: 0.0\n",
      "FH: 0.8256880733944955 FFA: 0.0\n",
      "FH: 0.8534031413612565 FFA: 0.0\n",
      "FH: 0.8042328042328042 FFA: 0.014925373134328358\n",
      "FH: 0.9293478260869565 FFA: 0.0\n",
      "FH: 0.9818181818181818 FFA: 0.07534246575342465\n",
      "FH: 0.9375 FFA: 0.109375\n",
      "FH: 0.9553072625698324 FFA: 0.03896103896103896\n",
      "FH: 0.95625 FFA: 0.020833333333333332\n",
      "FH: 0.9631336405529954 FFA: 0.0\n",
      "FH: 0.8492462311557789 FFA: 0.0\n",
      "FH: 0.9393939393939394 FFA: 0.01098901098901099\n",
      "FH: 0.9550561797752809 FFA: 0.0\n",
      "FH: 0.934640522875817 FFA: 0.04854368932038835\n",
      "FH: 0.9246231155778895 FFA: 0.0\n",
      "FH: 0.8162162162162162 FFA: 0.028169014084507043\n",
      "FH: 0.9632352941176471 FFA: 0.041666666666666664\n",
      "FH: 0.9513513513513514 FFA: 0.0\n",
      "FH: 0.9867549668874173 FFA: 0.047619047619047616\n",
      "FH: 0.9152542372881356 FFA: 0.13924050632911392\n",
      "FH: 1.0 FFA: 0.02247191011235955\n",
      "FH: 0.9924242424242424 FFA: 0.08870967741935484\n",
      "FH: 0.9204545454545454 FFA: 0.0\n",
      "Checking accuracy on Test Set\n",
      "FH: 0.9711538461538461 FFA: 0.2565789473684211\n",
      "FH: 1.0 FFA: 0.2893401015228426\n",
      "FH: 0.8547008547008547 FFA: 0.5035971223021583\n",
      "FH: 0.8931297709923665 FFA: 0.472\n",
      "FH: 0.9090909090909091 FFA: 0.39285714285714285\n",
      "FH: 0.9024390243902439 FFA: 0.43609022556390975\n",
      "FH: 0.8802816901408451 FFA: 0.3157894736842105\n",
      "FH: 0.9347826086956522 FFA: 0.3353658536585366\n",
      "FH: 0.9113924050632911 FFA: 0.3469387755102041\n",
      "FH: 0.896 FFA: 0.3816793893129771\n",
      "FH: 0.8790322580645161 FFA: 0.30303030303030304\n",
      "FH: 0.922077922077922 FFA: 0.3333333333333333\n",
      "FH: 0.888 FFA: 0.5190839694656488\n",
      "FH: 0.9050632911392406 FFA: 0.2755102040816326\n",
      "FH: 0.9037037037037037 FFA: 0.6033057851239669\n",
      "FH: 0.9212598425196851 FFA: 0.5116279069767442\n",
      "FH: 0.9041095890410958 FFA: 0.4909090909090909\n",
      "FH: 0.9 FFA: 0.3484848484848485\n",
      "FH: 0.9620253164556962 FFA: 0.23163841807909605\n",
      "FH: 0.8991596638655462 FFA: 0.34306569343065696\n",
      "FH: 0.9230769230769231 FFA: 0.3793103448275862\n",
      "FH: 0.8346456692913385 FFA: 0.3643410852713178\n",
      "FH: 0.8758169934640523 FFA: 0.5242718446601942\n",
      "FH: 0.9212598425196851 FFA: 0.5348837209302325\n",
      "FH: 0.9435483870967742 FFA: 0.3106060606060606\n",
      "FH: 0.9337748344370861 FFA: 0.44761904761904764\n",
      "FH: 0.8774193548387097 FFA: 0.2079207920792079\n",
      "FH: 0.8933333333333333 FFA: 0.3867924528301887\n",
      "FH: 0.8980891719745223 FFA: 0.46464646464646464\n",
      "FH: 0.8721804511278195 FFA: 0.15447154471544716\n",
      "FH: 0.9603960396039604 FFA: 0.5741935483870968\n",
      "FH: 0.9711538461538461 FFA: 0.3157894736842105\n",
      "FH: 0.8939393939393939 FFA: 0.3790322580645161\n",
      "FH: 0.9407407407407408 FFA: 0.39669421487603307\n",
      "FH: 0.9504132231404959 FFA: 0.21481481481481482\n",
      "FH: 0.9333333333333333 FFA: 0.39669421487603307\n",
      "NOISY\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n",
      "FH: 0.7297297297297297\n",
      "FFA: 0.6413793103448275\n",
      "FH: 0.8505747126436781\n",
      "FFA: 0.591715976331361\n",
      "FH: 0.8770949720670391\n",
      "FFA: 0.22077922077922077\n",
      "FH: 0.8245614035087719\n",
      "FFA: 0.38823529411764707\n",
      "FH: 0.6772151898734177\n",
      "FFA: 0.6836734693877551\n",
      "FH: 0.7440476190476191\n",
      "FFA: 0.5568181818181818\n",
      "FH: 0.6473988439306358\n",
      "FFA: 0.7469879518072289\n",
      "FH: 0.9124087591240876\n",
      "FFA: 0.4117647058823529\n",
      "FH: 0.711340206185567\n",
      "FFA: 0.5806451612903226\n",
      "FH: 0.679144385026738\n",
      "FFA: 0.6811594202898551\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 256),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "\n",
    "def predict(inputVector, weightMatrix):\n",
    "        '''\n",
    "        Predict the output of the input vector.\n",
    "        '''\n",
    "        \n",
    "        predictionVector = []\n",
    "        for i in range(len(weightMatrix)):\n",
    "            row_sum = sum(\n",
    "                (inputVector[j] * weightMatrix[j][i])\n",
    "                for j in range(len(weightMatrix[0]) - 1)\n",
    "            )\n",
    "\n",
    "            predictionVector.append(self.activation_fn(row_sum))\n",
    "           \n",
    "        return predictionVector\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, inputImageVectors):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    tableObject = {}\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "\n",
    "    for i in range(len(stdev)):\n",
    "        tableObject['std_'+ str(stdev[i]) + '_fh'] = []\n",
    "        tableObject['std_'+ str(stdev[i]) + '_ffa'] = []\n",
    "\n",
    "    for j in range(len(stdev)):   \n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],0.1,stdev[j]) \n",
    "            testPrediction = model(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if output[l] > 0:\n",
    "                    testPrediction[l] = 1\n",
    "                else:\n",
    "                    testPrediction[l] = 0\n",
    "            fh,ffa = calculate_performance_metrics(inputImageVectors[k],testPrediction)\n",
    "            print(\"FH:\",fh)\n",
    "            print(\"FFA:\",ffa)\n",
    "\n",
    "ImageVectors = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),'characters1', str(i) +'.png')\n",
    "    im = Image.open(path, 'r')\n",
    "    gray = im.convert('L')\n",
    "    bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "    ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "for i in range(26):\n",
    "    x = i + 65\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),'characters1', chr(i + 65) +'.png')\n",
    "    im = Image.open(path, 'r')\n",
    "    gray = im.convert('L')\n",
    "    bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "    ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "set1 = np.array(ImageVectors)\n",
    "\n",
    "ImageVectors = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),'characters2', str(i) +'.png')\n",
    "    im = Image.open(path, 'r')\n",
    "    gray = im.convert('L')\n",
    "    bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "    ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "for i in range(26):\n",
    "    x = i + 65\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),'characters2', chr(i + 65) +'.png')\n",
    "    im = Image.open(path, 'r')\n",
    "    gray = im.convert('L')\n",
    "    bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "    ImageVectors.append(np.array(list(bw.getdata())))\n",
    "    \n",
    "set2 = np.array(ImageVectors)\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, imageTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "def calculate_performance_metrics(inputVector, outputVector):\n",
    "    totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "    totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    for i in range(256):\n",
    "        if outputVector[i] < 0.0001:\n",
    "            if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                rightBlackPixelCount += 1\n",
    "            else:\n",
    "                wrongBlackPixelCount += 1\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa \n",
    "\n",
    "model.eval()\n",
    "print(\"Checking accuracy on Training Set\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"Checking accuracy on Test Set\")\n",
    "for i in range(36):\n",
    "    output = model(torch.from_numpy(set2[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(256):\n",
    "        if output[j] > 0:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    fh, ffa = calculate_performance_metrics(set2[i], output)\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "print(\"NOISY\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev, 10, set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895ada0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
