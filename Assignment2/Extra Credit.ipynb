{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class StageOne(nn.Module):\n",
    "    '''\n",
    "    Stage One of Two Stage DNN.\n",
    "  '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(64,32),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64,16),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n",
    "class StageTwo(nn.Module):\n",
    "    '''\n",
    "    Stage Two of Two Stage DNN.\n",
    "  '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(16, 16),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(16,512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512,256),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(256,256),\n",
    "        #nn.ReLU(),\n",
    "        #nn.Linear(256,256),\n",
    "        nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)  \n",
    "\n",
    "\n",
    "\n",
    "def add_noise(outputVector,noise_percent,stdev):\n",
    "    '''\n",
    "    Add noise to the output vector.\n",
    "    '''\n",
    "    mean = 0\n",
    "    noise = np.random.normal(mean,stdev,outputVector.shape) * noise_percent\n",
    "    return outputVector + noise\n",
    "\n",
    "def Noisy_Testing(stdev, testRounds, noiseCrossSection, stageOneModel, stageTwoModel, inputImageVectors, outputVector):\n",
    "    '''\n",
    "    Test the DNN with noise.\n",
    "    '''\n",
    "    noisySetDisplayArray=[[0]*2*len(stdev) for i in range(testRounds)]\n",
    "    plotObject = {'fh': [], 'ffa': []}\n",
    "    for j in range(len(stdev)):\n",
    "        for k in range(testRounds) :\n",
    "            corruptedVector = add_noise(inputImageVectors[k],noiseCrossSection,stdev[j])\n",
    "            noisySetDisplayArray[k][2*j] = getPredictedImage(corruptedVector) \n",
    "            stageOnePrediction = stageOneModel(torch.from_numpy(corruptedVector.astype('float32'))).detach().numpy()\n",
    "            stageTwoPrediction = stageTwoModel(torch.from_numpy(stageOnePrediction.astype('float32'))).detach().numpy()\n",
    "            for l in range(256):\n",
    "                if stageTwoPrediction[l] > 0.1:\n",
    "                    stageTwoPrediction[l] = 1\n",
    "                else:\n",
    "                    stageTwoPrediction[l] = 0\n",
    "            noisySetDisplayArray[k][(2*j)+1] = getPredictedImage(stageTwoPrediction)\n",
    "            fh,ffa = calculate_performance_metrics(outputVector[k],stageTwoPrediction,'default')\n",
    "            plotObject['fh'].append(round(fh,2))\n",
    "            plotObject['ffa'].append(round(ffa,2))\n",
    "\n",
    "    x = np.array([\n",
    "        0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
    "        0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
    "        0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002,\n",
    "        0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003,\n",
    "        0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005,\n",
    "        0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "        0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n",
    "        0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
    "        0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "        0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,0.1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "        \n",
    "    ax.scatter(x, plotObject['fh'], label=\"Fh\", marker= \"^\", color=\"orange\", s=30)\n",
    "    ax.scatter(x, plotObject['ffa'], label=\"Ffa\", marker= \"^\", color=\"red\", s=30)\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_formatter(tick.FormatStrFormatter('%g'))\n",
    "    \n",
    "    \n",
    "    plt.xlabel('Gaussian Noise Level (stdev, at ' + str(noiseCrossSection*100) + ' pct xsecn)')\n",
    "    plt.ylabel(\"$\\it{Fh}$ and $\\it{Ffa}$\")\n",
    "    plt.xticks(x)\n",
    "    plt.title('Graph of Fh and Ffa vs. Noise (Cross-section '+ str(int(noiseCrossSection*100)) + '%) Standard Deviation \\n for noise-corrupted Alphanumeric Imagery (16x16 pixels) for \\n Heteroassociative Deep Neural Network')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    displayNoisyPredictions(noisySetDisplayArray,stdev,noiseCrossSection)\n",
    "            \n",
    "\n",
    "def Create_Image_Set(filename, ASL):\n",
    "    ImageVectors = []\n",
    "    if filename!='ASL32x' and filename!='ASL64x' and ASL==False: #set1 and set2\n",
    "        for i in range(10):\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, str(i) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "        for i in range(26):\n",
    "            x = i + 65\n",
    "            path = os.path.join(os.path.dirname(os.path.abspath(sys.argv[1])),filename, chr(i + 65) +'.png')\n",
    "            im = Image.open(path, 'r')\n",
    "            gray = im.convert('L')\n",
    "            bw = gray.point(lambda x: 0 if x<135 else 1, '1')\n",
    "            ImageVectors.append(np.array(list(bw.getdata())))\n",
    "\n",
    "    return np.array(ImageVectors)\n",
    "\n",
    "def calculate_performance_metrics(inputVector, outputVector, context):\n",
    "    if context == 'semantics':\n",
    "        totalBlackPixelCount = sum(x == 1 for x in inputVector)\n",
    "        totalWhitePixelCount = sum(x == 0 for x in inputVector)\n",
    "    elif context == 'default':\n",
    "        totalBlackPixelCount = sum(x == 0 for x in inputVector)\n",
    "        totalWhitePixelCount = sum(x == 1 for x in inputVector)\n",
    "\n",
    "    wrongBlackPixelCount = 0\n",
    "    rightBlackPixelCount = 0\n",
    "    \n",
    "    for i in range(len(inputVector)):\n",
    "        if context == 'semantics':\n",
    "            if outputVector[i] == 1:\n",
    "                if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                    rightBlackPixelCount += 1\n",
    "                else:\n",
    "                    wrongBlackPixelCount += 1\n",
    "        elif context == 'default':\n",
    "            if outputVector[i] < 0.001:\n",
    "                if  abs(outputVector[i] - inputVector[i]) < 0.0001:\n",
    "                    rightBlackPixelCount += 1\n",
    "                else:\n",
    "                    wrongBlackPixelCount += 1  \n",
    "\n",
    "    fh = rightBlackPixelCount/totalBlackPixelCount\n",
    "    ffa = wrongBlackPixelCount/totalWhitePixelCount\n",
    "    return fh, ffa\n",
    "    \n",
    "def getPredictedImage(predictionVector):\n",
    "        '''\n",
    "        Get the predicted image.\n",
    "        '''\n",
    "        tempArray = []\n",
    "        for i in range(len(predictionVector)):\n",
    "            if predictionVector[i] < 0.0001:\n",
    "                tempArray.append((0,0,0)) \n",
    "            else:\n",
    "                tempArray.append((255,255,255))  \n",
    "        row_length = int(math.sqrt(len(predictionVector)))\n",
    "        finalImage = []\n",
    "        for i in range(0,len(tempArray),row_length):\n",
    "            finalImage.append(tempArray[i:i+(row_length-1)])\n",
    "        imageArray = np.array(finalImage, dtype=np.uint8)\n",
    "        return Image.fromarray(imageArray)\n",
    "\n",
    "def displayNoisyPredictions(predictionDataArray, stddev, noiseCrossSection):\n",
    "    '''\n",
    "    Display the training predictions.\n",
    "    '''\n",
    "    f,axisArray = plt.subplots(len(predictionDataArray),len(predictionDataArray[0]),figsize=(30,30))\n",
    "    plt.suptitle('Two Stage DNN Noisy Set performance with Noise Cross section '+ str(int(noiseCrossSection*100)) + '%', fontsize = 16)\n",
    "    for i in range(len(predictionDataArray)):\n",
    "        for j in range(len(predictionDataArray[0])):\n",
    "            axisArray[i,j].axes.xaxis.set_visible(False)\n",
    "            axisArray[i,j].axes.yaxis.set_visible(False)\n",
    "            if j % 2 == 0:\n",
    "                axisArray[i,j].set_title('Std. dev '+str(stddev[int(j/2)]), fontsize=9,x =0.5,y = 0.95)\n",
    "            else:\n",
    "                axisArray[i,j].set_title('Output', fontsize=9, x =0.5,y = 0.95)\n",
    "            axisArray[i,j].imshow(predictionDataArray[i][j])\n",
    "\n",
    "set1 = Create_Image_Set('characters1',False)\n",
    "set2 = Create_Image_Set('characters2',False)\n",
    "'''\n",
    "    Sematic Representation of Dataset.\n",
    "'''\n",
    "'''\n",
    "Sematic Representation Details:\n",
    "Every character is represented as a 16-element array indicating the presence/absence of 16 different constituent elements\n",
    "1st element - Large Upper left arc (0: absent, 1: present)\n",
    "2nd element - Small Upper left arc (0: absent, 1: present)\n",
    "3rd element - Large Upper right arc (0: absent, 1: present)\n",
    "4th element - Small Upper right arc (0: absent, 1: present)\n",
    "5th element - Large Lower left arc (0: absent, 1: present)\n",
    "6th element - Small Lower left arc (0: absent, 1: present)\n",
    "7th element - Large Lower right arc (0: absent, 1: present)\n",
    "8th element - Small Lower right arc (0: absent, 1: present)\n",
    "9th element - Large Backward slash (0: absent, 1: present)\n",
    "10th element - Small Backward slash (0: absent, 1: present)\n",
    "11th element - Large Forward slash (0: absent, 1: present)\n",
    "12th element - Small Forward slash (0: absent, 1: present)\n",
    "13th element - Large Vertical line (0: absent, 1: present)\n",
    "14th element - Small Vertical line (0: absent, 1: present)\n",
    "15th element - Large Horizontal line (0: absent, 1: present)\n",
    "16th element - Small Horizontal line (0: absent, 1: present)\n",
    "'''\n",
    "outputArray = [ [0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,0], #0\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0], #1\n",
    "                [0,1,0,1,0,0,0,0,0,0,1,0,0,0,1,0], #2\n",
    "                [0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,0], #3\n",
    "                [0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0], #4\n",
    "                [0,0,0,0,0,1,0,1,0,0,0,0,0,1,1,0], #5\n",
    "                [1,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0], #6\n",
    "                [0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0], #7\n",
    "                [0,1,0,1,0,1,0,1,1,0,1,0,0,0,0,0], #8\n",
    "                [0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0], #9\n",
    "                [0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1], #A\n",
    "                [0,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0], #B\n",
    "                [1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0], #C\n",
    "                [0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0], #D\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1], #E\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1], #F\n",
    "                [1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1], #G\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0], #H\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1], #I\n",
    "                [0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,0], #J\n",
    "                [0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0], #K\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0], #L\n",
    "                [0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0], #M\n",
    "                [0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0], #N\n",
    "                [1,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0], #O\n",
    "                [0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0], #P\n",
    "                [1,0,1,0,1,0,1,0,0,1,0,0,0,0,0,0], #Q\n",
    "                [0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0], #R\n",
    "                [0,1,0,1,0,1,0,1,0,1,0,0,0,0,0,0], #S\n",
    "                [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1], #T\n",
    "                [0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0], #U\n",
    "                [0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0], #V\n",
    "                [0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0], #W\n",
    "                [0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0], #X\n",
    "                [0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0], #Y\n",
    "                [0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1] ] #Z\n",
    "\n",
    "imageTensor = torch.Tensor(set1)\n",
    "resultTensor = torch.Tensor(outputArray)\n",
    "_dataSet = TensorDataset(imageTensor, resultTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "stageOneModel = StageOne()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(stageOneModel.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"\\n------EXTRA CREDIT - Stage 1------\\n\")\n",
    "\n",
    "secondStageInput=[]\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = stageOneModel(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "for i in range(36):\n",
    "    output = stageOneModel(torch.from_numpy(set1[i].astype('float32'))).detach().numpy()\n",
    "    for j in range(16):\n",
    "        if output[j] > 0.01:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    # print(\"Output: \",output)\n",
    "    fh, ffa = calculate_performance_metrics(outputArray[i], output,'semantics')\n",
    "    # plt.figure()\n",
    "    #plt.imshow(np.reshape(output,[16,16]))\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "    secondStageInput.append(output)\n",
    "    \n",
    "# model.eval()\n",
    "\n",
    "print(\"\\n------EXTRA CREDIT - Stage 2------\\n\")\n",
    "\n",
    "imageTensor = torch.Tensor(secondStageInput)\n",
    "resultTensor = torch.Tensor(set1)\n",
    "_dataSet = TensorDataset(imageTensor, resultTensor)\n",
    "_dataLoader = DataLoader(_dataSet)\n",
    "\n",
    "stageTwoModel = StageTwo()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(stageTwoModel.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(_dataLoader):\n",
    "\n",
    "        # forward\n",
    "        scores = stageTwoModel(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "for i in range(36):\n",
    "    output = stageTwoModel(torch.from_numpy(secondStageInput[i].astype('float32'))).detach().numpy()\n",
    "    # print(\"Input:\",set1[i],\"Output:\",output)\n",
    "    for j in range(256):\n",
    "        if output[j] > 0.1:\n",
    "            output[j] = 1\n",
    "        else:\n",
    "            output[j] = 0\n",
    "    # print(\"Output: \",output)\n",
    "    fh, ffa = calculate_performance_metrics(set1[i], output,'default')\n",
    "    # plt.figure()\n",
    "    # plt.imshow(np.reshape(output,[16,16]))\n",
    "    print(\"FH:\", fh, \"FFA:\", ffa)\n",
    "\n",
    "print(\"\\n------EXTRA CREDIT - Noisy Testing------\\n\")\n",
    "stdev = [0,0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05,0.1]\n",
    "Noisy_Testing(stdev,36,0.1,stageOneModel,stageTwoModel,set1,set1)\n",
    "Noisy_Testing(stdev,36,0.2,stageOneModel,stageTwoModel,set1,set1)\n",
    "Noisy_Testing(stdev,36,0.25,stageOneModel,stageTwoModel,set1,set1)\n",
    "Noisy_Testing(stdev,36,0.3,stageOneModel,stageTwoModel,set1,set1)\n",
    "Noisy_Testing(stdev,36,0.35,stageOneModel,stageTwoModel,set1,set1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47eafa600f66963ab59f1dc7b3b5a1ad344b808e29e3b1eef3a9643311b98194"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
